{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Libraries + **MODEL** (EleutherAI/pythia-70m-deduped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.dictionary import IdentityDict\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "# model hyperparameters\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "activation_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "+ Ambiguous/balanced set:  \n",
    "    + The ambiguous set, consisting of bios of male professors (labeled 0) and female nurses (labeled 1).  \n",
    "    + The balanced set, consisting of an equal number of bios for male professors, male nurses, female professors, and female nurses.  \n",
    "  \n",
    "+ Train/test  \n",
    "  \n",
    "+ Intended label [profession] (0) / Unintended label [gender] (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset hyperparameters\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "# data preparation hyperparameters\n",
    "batch_size = 1024\n",
    "SEED = 42\n",
    "\n",
    "# To fit on 24GB VRAM GPU, I set the next 2 default batch_sizes to 64\n",
    "def get_data(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    \"\"\"\n",
    "    Loads and processes the dataset for training or testing.\n",
    "\n",
    "    Parameters:\n",
    "    train (bool): If True, loads the training set; otherwise, loads the test set. Default is True.\n",
    "    ambiguous (bool): If True, loads the ambiguous set; otherwise, loads the balanced set. Default is True.\n",
    "    batch_size (int): The size of each batch. Default is 128.\n",
    "    seed (int): The random seed for shuffling the data. Default is SEED.\n",
    "\n",
    "    Returns:\n",
    "    list of tuples: A list of batches, where each batch is a tuple containing:\n",
    "        - data (list): A list of text data (bios).\n",
    "        - true_labels (torch.Tensor): A tensor of intended labels (profession).\n",
    "        - spurious_labels (torch.Tensor): A tensor of unintended labels (gender).\n",
    "    \"\"\"\n",
    "    #• The ambiguous set, consisting of bios of male professors (labeled 0) and female nurses (labeled 1).\n",
    "    #• The balanced set, consisting of an equal number of bios for male professors, male nurses, female professors, and female nurses.\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    \"\"\"\n",
    "    Generates subgroups of text data based on gender and profession labels.\n",
    "    Parameters:\n",
    "    train (bool): If True, use training data; otherwise, use test data. Default is True.\n",
    "    ambiguous (bool): If True, create two subgroups (ambiguous); otherwise, create four subgroups (non-ambiguous). Default is True.\n",
    "    batch_size (int): The size of each batch of data. Default is 128.\n",
    "    seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are label profiles (tuples) and values are lists of batches. Each batch is a tuple containing:\n",
    "        - A list of text data.\n",
    "        - A tensor of the first label repeated for the batch size.\n",
    "        - A tensor of the second label repeated for the batch size.\n",
    "    \"\"\"\n",
    "\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_labels, pos_labels = (0, 0), (1, 1)\n",
    "        subgroups = [(neg, neg_labels), (pos, pos_labels)]\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "        subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "    \n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probe - Linear classifier\n",
    "Train and Test  \n",
    "+ get activation form the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe training hyperparameters\n",
    "\n",
    "layer = 4 # model layer for attaching linear classification head (second-last layer)\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to(DEVICE)\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1] \n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses\n",
    "\n",
    "def test_probe(probe, get_acts, label_idx=0, batches=get_data(train=False), seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.0).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()\n",
    "    \n",
    "def get_acts(text):\n",
    "    with t.no_grad(): \n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            attn_mask = model.input[1]['attention_mask']\n",
    "            acts = model.gpt_neox.layers[layer].output[0]\n",
    "            acts = acts * attn_mask[:, :, None]\n",
    "            acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle  \n",
    "A classifier trained on ground-truth labels on the <u>**balanced**</u> set  \n",
    "Intended (ground-truth) labels [profession] (label_idx=0) / Unintended label [gender] (label_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous test accuracy 0.9318076372146606\n",
      "ground truth accuracy: 0.9302995204925537\n",
      "unintended feature accuracy: 0.49366357922554016\n"
     ]
    }
   ],
   "source": [
    "oracle, _ = train_probe(get_acts, label_idx=0, batches=get_data(ambiguous=False)) # train on balanced training set\n",
    "print(\"ambiguous test accuracy\", test_probe(oracle, get_acts, label_idx=0)) # test on ambiguous test set\n",
    "batches = get_data(train=False, ambiguous=False) # balanced test set\n",
    "print(\"ground truth accuracy:\", test_probe(oracle, get_acts, batches=batches, label_idx=0))\n",
    "print(\"unintended feature accuracy:\", test_probe(oracle, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9468682408332825\n",
      "Accuracy for (0, 1): 0.926398754119873\n",
      "Accuracy for (1, 0): 0.9239631295204163\n",
      "Accuracy for (1, 1): 0.9189126491546631\n"
     ]
    }
   ],
   "source": [
    "# get worst-group accuracy of oracle probe\n",
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "# label_profile: (profession (professor/nurse), gender (male/female)) [but I don't know in what order]\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(oracle, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probe\n",
    "A classifier trained on ground-truth labels on the <u>**ambiguous**</u> set  \n",
    "Intended (ground-truth) labels [profession] (label_idx=0) / Unintended label [gender] (label_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9955855011940002\n",
      "Ground truth accuracy: 0.6186636090278625\n",
      "Unintended feature accuracy: 0.8744239807128906\n"
     ]
    }
   ],
   "source": [
    "probe, _ = train_probe(get_acts, label_idx=0)\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9978401064872742\n",
      "Accuracy for (0, 1): 0.24355988204479218\n",
      "Accuracy for (1, 0): 0.2626728117465973\n",
      "Accuracy for (1, 1): 0.9934943914413452\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary - SAEs + metric_fn\n",
    "<img src=\"../images/LLM_schema.png\" alt=\"LLM_schema\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of components, that is num of SAEs: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of components, that is num of SAEs:\", 1+(layer + 1)*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dictionaries\n",
    "\n",
    "# dictionary hyperparameters\n",
    "dict_id = 10\n",
    "expansion_factor = 64\n",
    "dictionary_size = expansion_factor * activation_dim\n",
    "\n",
    "submodules = []\n",
    "dictionaries = {}\n",
    "\n",
    "submodules.append(model.gpt_neox.embed_in)\n",
    "dictionaries[model.gpt_neox.embed_in] = AutoEncoder.from_pretrained(\n",
    "    f'../dictionaries/pythia-70m-deduped/embed/{dict_id}_{dictionary_size}/ae.pt',\n",
    "    device=DEVICE\n",
    ")\n",
    "for i in range(layer + 1):\n",
    "    submodules.append(model.gpt_neox.layers[i].attention)\n",
    "    dictionaries[model.gpt_neox.layers[i].attention] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/attn_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i].mlp)\n",
    "    dictionaries[model.gpt_neox.layers[i].mlp] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/mlp_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i])\n",
    "    dictionaries[model.gpt_neox.layers[i]] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/resid_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "#probe is the linear classifier trained on ambiguous training set\n",
    "def metric_fn(model, labels=None):\n",
    "    attn_mask = model.input[1]['attention_mask']\n",
    "    acts = model.gpt_neox.layers[layer].output[0]\n",
    "    acts = acts * attn_mask[:, :, None]\n",
    "    acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "    \n",
    "    return t.where(\n",
    "        labels == 0,\n",
    "        probe(acts),\n",
    "        - probe(acts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most influential features\n",
    "Patching effect  \n",
    "ig = integrated gradient  \n",
    "  \n",
    "+ <u>Attribution patching</u>: activation patching at industrial scale, it uses gradients to take linear approximation to activation patching\n",
    "+ <u>**Integrated gradients**</u>: a more expensive but more accurate approximation (applicable since we use small models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:09<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# find most influential features\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, labels, _) in tqdm(enumerate(get_data(train=True, ambiguous=True, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold\n",
    "We apply some choice of node\n",
    "threshold TN to select nodes with a large (absolute) IE.  \n",
    "To keep the number of nodes we need to annotate manageable, we set a relatively high\n",
    "node threshold of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "946 0.22117570042610168\n",
      "5719 0.14886397123336792\n",
      "7392 0.3218412399291992\n",
      "10784 0.15049311518669128\n",
      "17846 0.31437844038009644\n",
      "22068 0.1622835248708725\n",
      "23079 0.15247352421283722\n",
      "25904 0.10423339903354645\n",
      "28533 0.18998156487941742\n",
      "29476 0.19172973930835724\n",
      "31461 0.16866040229797363\n",
      "31467 0.16690056025981903\n",
      "32081 0.32751068472862244\n",
      "32469 1.3901286125183105\n",
      "Component 1:\n",
      "23752 0.10792490839958191\n",
      "Component 2:\n",
      "2995 0.10343753546476364\n",
      "3842 0.14583298563957214\n",
      "10258 0.29746800661087036\n",
      "13387 0.14572963118553162\n",
      "13968 0.12934660911560059\n",
      "18382 0.2589782476425171\n",
      "19369 0.18766078352928162\n",
      "28127 1.0541187524795532\n",
      "30518 0.19712230563163757\n",
      "Component 3:\n",
      "1022 0.3159206211566925\n",
      "9651 0.5887967348098755\n",
      "10060 2.43733549118042\n",
      "18967 0.6829056143760681\n",
      "22084 0.2536494731903076\n",
      "23898 0.4724956154823303\n",
      "24799 0.10279475897550583\n",
      "26504 0.3044505715370178\n",
      "29626 0.2844652533531189\n",
      "31201 0.17182287573814392\n",
      "Component 4:\n",
      "8147 0.10606715828180313\n",
      "Component 5:\n",
      "24159 0.24803251028060913\n",
      "25018 0.4943896234035492\n",
      "Component 6:\n",
      "4592 0.3920895755290985\n",
      "8920 0.5931018590927124\n",
      "9877 0.3653258681297302\n",
      "12128 0.6474209427833557\n",
      "15017 3.1611530780792236\n",
      "17369 0.10423677414655685\n",
      "26969 0.10391923785209656\n",
      "30248 1.0404223203659058\n",
      "Component 7:\n",
      "13570 0.1257641464471817\n",
      "27472 1.238729476928711\n",
      "Component 8:\n",
      "Component 9:\n",
      "1995 1.0039465427398682\n",
      "9128 1.5338925123214722\n",
      "11656 0.1635110080242157\n",
      "12440 0.18845383822917938\n",
      "14638 0.15359634160995483\n",
      "29206 0.21679852902889252\n",
      "29295 0.6558160781860352\n",
      "31098 0.11044643074274063\n",
      "Component 10:\n",
      "2959 0.9745623469352722\n",
      "19128 0.26585495471954346\n",
      "22029 0.17818133533000946\n",
      "Component 11:\n",
      "Component 12:\n",
      "19558 1.547678828239441\n",
      "23545 0.3753567934036255\n",
      "24806 0.12211156636476517\n",
      "27334 0.15601801872253418\n",
      "31453 0.17198815941810608\n",
      "Component 13:\n",
      "31101 0.5748973488807678\n",
      "Component 14:\n",
      "Component 15:\n",
      "9766 0.23256155848503113\n",
      "12420 1.3541486263275146\n",
      "30220 0.2663297951221466\n",
      "total features: 67\n"
     ]
    }
   ],
   "source": [
    "n_features = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    for idx in (effect > 0.1).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        n_features += 1\n",
    "print(f\"total features: {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANUALLY Examine most influential features  \n",
    "Manually inspect and evaluate for task-relevancy each feature in the circuit from\n",
    "previous Step. For each feature, examine the text data that activates the feature the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde341f7206a4e31bdb2281fc2cbfa6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/feature_circuit_ari/env_fc/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' nursing', 5.101802825927734), (' Nursing', 3.9356651306152344), (' nurse', 2.8416316509246826), (' nurses', 2.8025169372558594), (' RN', 1.5293744802474976), (' Teaching', 0.7016171216964722), (' rehabilitation', 0.6920690536499023), ('wife', 0.6749840974807739), ('unte', 0.6074658632278442), (' sewing', 0.6034237146377563), (' caring', 0.4292561411857605), ('ancy', 0.3882577419281006), ('akers', 0.38148069381713867), (' lending', 0.34468239545822144), (' volunteers', 0.3259945511817932), (' drinking', 0.2667090892791748), (' Clinical', 0.26244115829467773), (' inpatient', 0.23919767141342163), (' architect', 0.22628939151763916), (' Medical', 0.21756377816200256), (' relational', 0.19916176795959473), (' Dou', 0.19640305638313293), (' dialysis', 0.1761435568332672), (' Leadership', 0.1721816062927246), ('hin', 0.14378538727760315), ('okin', 0.1413027048110962), (' executive', 0.11437027156352997), (' teaching', 0.11418899148702621), (' poet', 0.10126781463623047), (' hospitals', 0.0999029278755188)]\n",
      "[('�', 1.2042195796966553), ('��', 1.1853760480880737), ('�', 1.1379964351654053), ('�', 1.1257283687591553), ('�', 1.0499390363693237), (' home', 0.9965490698814392), (' homes', 0.9865149855613708), ('�', 0.9813761711120605), ('�', 0.9332275986671448), ('�', 0.9122070670127869), ('�', 0.8883179426193237), ('giving', 0.8878536224365234), ('�', 0.8529338240623474), ('�', 0.8425619006156921), ('�', 0.8414469957351685), ('��', 0.8361857533454895), ('��', 0.8251465559005737), ('�', 0.7906738519668579), ('�', 0.7814779281616211), ('��', 0.7766968011856079), ('�', 0.7744954824447632), ('�', 0.7614461779594421), ('��', 0.7600098252296448), ('�', 0.7405151724815369), ('�', 0.7403483390808105), ('�', 0.7380574941635132), (' practitioner', 0.7281612157821655), ('�', 0.7212402820587158), ('��', 0.7185587882995605), ('�', 0.7153890132904053)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-f7058eb6-679e\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-f7058eb6-679e\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\", \"\\n\", \"\\n\", \"Dr\", \".\", \" Gregory\", \" has\", \" Doctor\", \"ate\", \" of\", \" Nursing\", \" Practice\", \" in\", \" Health\", \" Innovation\", \" and\", \" Leadership\", \" from\", \" the\", \" University\", \" of\", \" Minnesota\", \" and\", \" a\", \" b\", \"achel\", \"ors\", \" in\", \" nursing\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\"], [\"A\", \" small\", \" city\", \" in\", \" Iowa\", \" has\", \" taken\", \" an\", \" action\", \" to\", \" save\", \" the\", \" bees\", \" from\", \" extinction\", \".\", \" Ac\", \"res\", \" of\", \" land\", \" were\", \" donated\", \" to\", \" increase\", \" the\", \" local\", \" habitats\", \" of\", \" the\", \" bees\", \".\", \"\\n\", \"\\n\", \"Over\", \" the\", \" past\", \" decade\", \",\", \" bees\", \" are\", \" steadily\", \" disappearing\", \".\", \" Work\", \"er\", \" bees\", \" disappear\", \" and\", \" leaving\", \" behind\", \" the\", \" queen\", \".\", \" With\", \" a\", \" few\", \" nursing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\", \"\\n\", \"\\n\", \"Dr\", \".\", \" Gregory\", \" has\", \" Doctor\", \"ate\", \" of\", \" Nursing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\", \" provision\", \" for\", \" nurses\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\"], [\"No\", \" other\", \" appliance\", \" company\", \" has\", \" a\", \" wider\", \" scope\", \" of\", \" solutions\", \",\", \" nor\", \" the\", \" experience\", \" to\", \" back\", \" them\", \" up\", \",\", \" than\", \" Elect\", \"rol\", \"ux\", \".\", \" Our\", \" long\", \" presence\", \" in\", \" people\", \"\\u2019\", \"s\", \" homes\", \" around\", \" the\", \" world\", \" means\", \" that\", \" no\", \" other\", \" appliance\", \" company\", \" ...\", \"\\n\", \"Read\", \" more\", \"\\n\", \"\\n\", \"...\", \" Easy\", \"-\", \"F\", \"lo\", \" vacu\", \"ums\", \" including\", \" parts\", \" and\", \" bags\", \".\", \" Find\", \"lay\", \"'s\", \" also\", \" offers\", \" sales\", \" and\", \" service\", \" for\", \" all\", \" makes\", \" and\", \" models\", \" of\", \" sewing\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\", \"-\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\"], [\"PCI\", \" Alternative\", \" Using\", \" S\", \"ust\", \"ained\", \" Exercise\", \" (\", \"PA\", \"USE\", \"):\", \" R\", \"ational\", \"e\", \" and\", \" trial\", \" design\", \".\", \"\\n\", \"Card\", \"i\", \"ovascular\", \" disease\", \" (\", \"C\", \"VD\", \")\", \" currently\", \" claims\", \" nearly\", \" one\", \" million\", \" lives\", \" yearly\", \" in\", \" the\", \" US\", \",\", \" accounting\", \" for\", \" nearly\", \" 40\", \"%\", \" of\", \" all\", \" deaths\", \".\", \" Coron\", \"ary\", \" artery\", \" disease\", \" (\", \"CAD\", \")\", \" accounts\", \" for\", \" the\", \" largest\", \" number\", \" of\", \" these\", \" deaths\", \".\", \" While\", \" efforts\", \" aimed\", \" at\", \" treating\", \" CAD\", \" in\", \" recent\", \" decades\", \" have\", \" concentrated\", \" on\", \" surgical\", \" and\", \" catheter\", \"-\", \"based\", \" interventions\", \",\", \" limited\", \" resources\", \" have\", \" been\", \" directed\", \" toward\", \" prevention\", \" and\", \" rehabilitation\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\", \"-\", \"mid\", \"wife\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\"], [\"Account\", \"ing\", \"\\n\", \"\\n\", \"Sur\", \"f\", \" Works\", \" offer\", \" a\", \" range\", \" of\", \" accounting\", \" services\", \" suitable\", \" for\", \" all\", \" types\", \" of\", \" business\", \".\", \" Below\", \",\", \" we\", \" have\", \" listed\", \" packages\", \" suitable\", \" for\", \" sole\", \" traders\", \",\", \" partnerships\", \" and\", \" limited\", \" companies\", \".\", \" The\", \" packages\", \" can\", \" be\", \" fully\", \" tailored\", \" to\", \" your\", \" requirements\", \" by\", \" adding\", \" extra\", \" services\", \" to\", \" create\", \" the\", \" exact\", \" service\", \" that\", \" you\", \" and\", \" your\", \" business\", \" requires\", \".\", \"\\n\", \"\\n\", \"All\", \" services\", \" are\", \" carried\", \" out\", \" on\", \" time\", \" with\", \" the\", \" minimum\", \" of\", \" fuss\", \" by\", \" our\", \" in\", \" house\", \",\", \" fully\", \" qualified\", \" accountant\", \"\\n\", \"\\n\", \"The\", \" list\", \" of\", \" services\", \" offered\", \" is\", \" not\", \" exhaustive\", \" so\", \" please\", \" let\", \" us\", \" know\", \" if\", \" you\", \" require\", \" a\", \" service\", \" not\", \" listed\", \".\", \" If\", \" you\", \" have\", \" specific\", \" needs\", \" we\", \" can\", \" build\", \" a\", \" bes\", \"p\", \"oke\", \" account\", \"ancy\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\", \" provision\", \" for\", \" nurses\", \" and\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"ers\", \" at\", \" CAM\", \"C\", \" bring\", \" their\", \" unique\", \" personalities\", \" and\", \" skills\", \" to\", \" our\", \" hospital\", \".\", \" They\", \" range\", \" in\", \" age\", \" from\", \" 15\", \" to\", \" 99\", \".\", \" Our\", \" ranks\", \" are\", \" made\", \" up\", \" of\", \" men\", \" and\", \" women\", \";\", \" students\", \" and\", \" retire\", \"es\", \";\", \" homem\", \"akers\"], [\"Got\", \" this\", \" cute\", \" little\", \" sewing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\", \"\\n\", \"\\n\", \"Dr\", \".\", \" Gregory\", \" has\", \" Doctor\", \"ate\", \" of\", \" Nursing\", \" Practice\", \" in\", \" Health\", \" Innovation\", \" and\", \" Leadership\", \" from\", \" the\", \" University\", \" of\", \" Minnesota\", \" and\", \" a\", \" b\", \"achel\", \"ors\"], [\"Micro\", \"-\", \"Lo\", \"an\", \" Program\", \"\\n\", \"\\n\", \"In\", \" order\", \" to\", \" promote\", \" economic\", \" development\", \" in\", \" the\", \" City\", \" of\", \" Al\", \"amo\", \",\", \" the\", \" Al\", \"amo\", \" ED\", \"C\", \" established\", \" the\", \" Al\", \"amo\", \" Small\", \" Business\", \" Micro\", \"-\", \"Lo\", \"an\", \" Program\", \" (\", \"ML\", \"P\", \")\", \" with\", \" assistance\", \" from\", \" USDA\", \" \\u2013\", \" Rural\", \" Development\", \".\", \" The\", \" M\", \"LP\", \" is\", \" a\", \" self\", \"-\", \"s\", \"ust\", \"aining\", \" project\", \" that\", \" works\", \" by\", \" lending\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\"], [\"[\", \"Central\", \" venous\", \" dialysis\", \" catheter\", \".\", \" Sil\", \"ic\", \"one\", \" rubber\", \" dialysis\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937150478363037]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431107997894287]], [[0.0]], [[0.0]], [[0.010102808475494385]], [[0.13302087783813477]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02793020009994507]], [[0.0]], [[3.9356651306152344]], [[0.0]], [[0.0]], [[0.0]], [[0.061070144176483154]], [[0.0]], [[0.1721816062927246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.35008299350738525]], [[0.0]], [[5.229028701782227]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174833297729492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.97146463394165]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.701827049255371]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937150478363037]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431107997894287]], [[0.0]], [[0.0]], [[0.010102808475494385]], [[0.13302087783813477]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02793020009994507]], [[0.0]], [[3.9356651306152344]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937150478363037]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.021908700466156006]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8095011711120605]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174833297729492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.97146463394165]], [[0.3862197995185852]], [[0.16560733318328857]], [[0.0]], [[0.0]], [[0.020936012268066406]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34256696701049805]], [[0.0]], [[0.0]], [[2.8025169372558594]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351275563240051]], [[0.0]], [[0.43192535638809204]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4292561411857605]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520798683166504]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8406283855438232]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.021908700466156006]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8095011711120605]], [[0.7401372194290161]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6920690536499023]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.021908700466156006]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8095011711120605]], [[0.7401372194290161]], [[0.0]], [[0.6749840974807739]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351275563240051]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351275563240051]], [[0.0]], [[0.43192535638809204]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351275563240051]], [[0.0]], [[0.43192535638809204]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4292561411857605]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3882577419281006]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174833297729492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.97146463394165]], [[0.3862197995185852]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174833297729492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.97146463394165]], [[0.3862197995185852]], [[0.16560733318328857]], [[0.0]], [[0.0]], [[0.020936012268066406]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34256696701049805]], [[0.0]], [[0.0]], [[2.8025169372558594]], [[0.38558393716812134]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351275563240051]], [[0.0]], [[0.43192535638809204]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4292561411857605]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520798683166504]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.38148069381713867]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.36621904373168945]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937150478363037]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431107997894287]], [[0.0]], [[0.0]], [[0.010102808475494385]], [[0.13302087783813477]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02793020009994507]], [[0.0]], [[3.9356651306152344]], [[0.0]], [[0.0]], [[0.0]], [[0.061070144176483154]], [[0.0]], [[0.1721816062927246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.35008299350738525]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34468239545822144]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937150478363037]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431107997894287]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174833297729492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.97146463394165]], [[0.3862197995185852]], [[0.16560733318328857]], [[0.0]], [[0.0]], [[0.020936012268066406]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34256696701049805]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.319751501083374]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7ff7f078ef50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interpret features\n",
    "\n",
    "# change the following two lines to pick which feature to interpret\n",
    "component_idx = 9\n",
    "feat_idx = 31098\n",
    "\n",
    "submodule = submodules[component_idx]\n",
    "dictionary = dictionaries[submodule]\n",
    "\n",
    "# interpret some features\n",
    "data = hf_dataset_to_generator(\"monology/pile-uncopyrighted\")\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    d_submodule=512,\n",
    "    refresh_batch_size=128, # decrease to fit on smaller GPUs\n",
    "    n_ctxs=512, # decrease to fit on smaller GPUs\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "out = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary,\n",
    "    dim_idx=feat_idx,\n",
    "    n_inputs=256 # decrease to fit on smaller GPUs\n",
    ")\n",
    "print(out.top_tokens)\n",
    "print(out.top_affected)\n",
    "out.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features to ablate: 55\n"
     ]
    }
   ],
   "source": [
    "feats_to_ablate = {\n",
    "    submodules[0] : [\n",
    "        946, # 'his'\n",
    "        # 5719, # 'research'\n",
    "        7392, # 'He'\n",
    "        # 10784, # 'Nursing'\n",
    "        17846, # 'He'\n",
    "        22068, # 'His'\n",
    "        # 23079, # 'tastes'\n",
    "        # 25904, # 'nursing'\n",
    "        28533, # 'She'\n",
    "        29476, # 'he'\n",
    "        31461, # 'His'\n",
    "        31467, # 'she'\n",
    "        32081, # 'her'\n",
    "        32469, # 'She'\n",
    "    ],\n",
    "    submodules[1] : [\n",
    "        # 23752, # capitalized words, especially pronouns\n",
    "    ],\n",
    "    submodules[2] : [\n",
    "        2995, # 'he'\n",
    "        3842, # 'She'\n",
    "        10258, # female names\n",
    "        13387, # 'she'\n",
    "        13968, # 'He'\n",
    "        18382, # 'her'\n",
    "        19369, # 'His'\n",
    "        28127, # 'She'\n",
    "        30518, # 'He'\n",
    "    ],\n",
    "    submodules[3] : [\n",
    "        1022, # 'she'\n",
    "        9651, # female names\n",
    "        10060, # 'She'\n",
    "        18967, # 'He'\n",
    "        22084, # 'he'\n",
    "        23898, # 'His'\n",
    "        # 24799, # promotes surnames\n",
    "        26504, # 'her'\n",
    "        29626, # 'his'\n",
    "        # 31201, # 'nursing'\n",
    "    ],\n",
    "    submodules[4] : [\n",
    "        # 8147, # unclear, something with names\n",
    "    ],\n",
    "    submodules[5] : [\n",
    "        24159, # 'She', 'she'\n",
    "        25018, # female names\n",
    "    ],\n",
    "    submodules[6] : [\n",
    "        4592, # 'her'\n",
    "        8920, # 'he'\n",
    "        9877, # female names\n",
    "        12128, # 'his'\n",
    "        15017, # 'she'\n",
    "        # 17369, # contact info\n",
    "        # 26969, # related to nursing\n",
    "        30248, # female names\n",
    "    ],\n",
    "    submodules[7] : [\n",
    "        13570, # promotes male-related words\n",
    "        27472, # female names, promotes female-related words\n",
    "    ],\n",
    "    submodules[8] : [\n",
    "    ],\n",
    "    submodules[9] : [\n",
    "        1995, # promotes female-associated words\n",
    "        9128, # feminine pronouns\n",
    "        11656, # promotes male-associated words\n",
    "        12440, # promotes female-associated words\n",
    "        # 14638, # related to contact information?\n",
    "        29206, # gendered pronouns\n",
    "        29295, # female names\n",
    "        # 31098, # nursing-related words\n",
    "    ],\n",
    "    submodules[10] : [\n",
    "        2959, # promotes female-associated words\n",
    "        19128, # promotes male-associated words\n",
    "        22029, # promotes female-associated words\n",
    "    ],\n",
    "    submodules[11] : [\n",
    "    ],\n",
    "    submodules[12] : [\n",
    "        19558, # promotes female-associated words\n",
    "        23545, # 'she'\n",
    "        24806, # 'her'\n",
    "        27334, # promotes male-associated words\n",
    "        31453, # female names\n",
    "    ],\n",
    "    submodules[13] : [\n",
    "        31101, # promotes female-associated words\n",
    "    ],\n",
    "    submodules[14] : [\n",
    "    ],\n",
    "    submodules[15] : [\n",
    "        9766, # promotes female-associated words\n",
    "        12420, # promotes female pronouns\n",
    "        30220, # promotes male pronouns\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Number of features to ablate: {sum(len(v) for v in feats_to_ablate.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting feats_to_ablate in a more useful format\n",
    "def n_hot(feats, dim=dictionary_size):\n",
    "    out = t.zeros(dim, dtype=t.bool, device=DEVICE)\n",
    "    for feat in feats:\n",
    "        out[feat] = True\n",
    "    return out\n",
    "\n",
    "feats_to_ablate = {\n",
    "    submodule : n_hot(feats) for submodule, feats in feats_to_ablate.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for ablating features\n",
    "is_tuple = {}\n",
    "with t.no_grad(), model.trace(\"_\"):\n",
    "    for submodule in submodules:\n",
    "        is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "def get_acts_ablated(\n",
    "    text,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate\n",
    "):\n",
    "\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            feat_idxs = to_ablate[submodule]\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x_hat, f = dictionary(x, output_features=True)\n",
    "            res = x - x_hat\n",
    "            f[...,feat_idxs] = 0. # zero ablation\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = dictionary.decode(f) + res\n",
    "            else:\n",
    "                submodule.output = dictionary.decode(f) + res\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "    return act.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy after ablating features judged irrelevant by human annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.926347553730011\n",
      "Ground truth accuracy: 0.8853686451911926\n",
      "Spurious accuracy: 0.5397465229034424\n"
     ]
    }
   ],
   "source": [
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, feats_to_ablate)\n",
    "\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison  \n",
    "Probe results:  \n",
    "Ambiguous test accuracy: 0.9955855011940002  \n",
    "Ground truth accuracy: 0.6186636090278625  \n",
    "Unintended feature accuracy: 0.8744239807128906 \n",
    "\n",
    "After ablating features:\n",
    "Ambiguous test accuracy: 0.926347553730011\n",
    "Ground truth accuracy: 0.8853686451911926\n",
    "Spurious accuracy: 0.5397465229034424\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9675408601760864\n",
      "Accuracy for (0, 1): 0.9309049844741821\n",
      "Accuracy for (1, 0): 0.7603686451911926\n",
      "Accuracy for (1, 1): 0.8849906921386719\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept bottleneck probing baseline\n",
    "from paper:  \n",
    "Concept Bottleneck Probing (CBP), adapted from Yan et al. (2023) (originally for\n",
    "multimodal text/image models). CBP works by training a probe to classify inputs\n",
    "x given access only to a vector of affinities between the LM’s representation of x\n",
    "and various concept vectors. See App. E.2 for implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = [    \n",
    "    ' nurse',\n",
    "    ' healthcare',\n",
    "    ' hospital',\n",
    "    ' patient',\n",
    "    ' medical',\n",
    "    ' clinic',\n",
    "    ' triage',\n",
    "    ' medication',\n",
    "    ' emergency',\n",
    "    ' surgery',\n",
    "    ' professor',\n",
    "    ' academia',\n",
    "    ' research',\n",
    "    ' university',\n",
    "    ' tenure',\n",
    "    ' faculty',\n",
    "    ' dissertation',\n",
    "    ' sabbatical',\n",
    "    ' publication',\n",
    "    ' grant',\n",
    "]\n",
    "# get concept vectors\n",
    "with t.no_grad(), model.trace(concepts):\n",
    "    concept_vectors = model.gpt_neox.layers[layer].output[0][:, -1, :].save()\n",
    "concept_vectors = concept_vectors.value - concept_vectors.value.mean(0, keepdim=True)\n",
    "\n",
    "def get_bottleneck(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        acts = model.gpt_neox.layers[layer].output[0]\n",
    "        acts = acts * attn_mask[:, :, None]\n",
    "        acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        # compute cosine similarity with concept vectors\n",
    "        sims = (acts @ concept_vectors.T) / (acts.norm(dim=-1)[:, None] @ concept_vectors.norm(dim=-1)[None])\n",
    "        sims = sims.save()\n",
    "    return sims.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth accuracy: 0.8335253596305847\n",
      "Unintended feature accuracy: 0.600806474685669\n"
     ]
    }
   ],
   "source": [
    "cbp_probe, _ = train_probe(get_bottleneck, label_idx=0, dim=len(concepts))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9276148676872253\n",
      "Accuracy for (0, 1): 0.7864062786102295\n",
      "Accuracy for (1, 0): 0.6774193644523621\n",
      "Accuracy for (1, 1): 0.9409851431846619\n"
     ]
    }
   ],
   "source": [
    "# get subgroup accuracies\n",
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline neuron performance\n",
    "The same as the feature skyline (below) but on neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:53<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# get neurons which are most influential for giving gender label\n",
    "neuron_dicts = {\n",
    "    submodule : IdentityDict(activation_dim).to(DEVICE) for submodule in submodules\n",
    "}\n",
    "\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        neuron_dicts,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "2 0.26396122574806213\n",
      "42 0.23957578837871552\n",
      "57 0.2323196977376938\n",
      "99 0.3270361125469208\n",
      "130 0.25990810990333557\n",
      "135 0.5472005605697632\n",
      "187 0.3968943953514099\n",
      "197 0.3398487865924835\n",
      "256 0.25377553701400757\n",
      "335 0.29094168543815613\n",
      "343 0.2429693192243576\n",
      "400 0.2538072466850281\n",
      "417 0.236003577709198\n",
      "421 0.24806241691112518\n",
      "Component 1:\n",
      "111 0.23795640468597412\n",
      "156 0.2911505103111267\n",
      "Component 2:\n",
      "23 0.32223719358444214\n",
      "156 0.39989417791366577\n",
      "Component 3:\n",
      "23 0.9157801866531372\n",
      "66 0.24019086360931396\n",
      "111 0.36800825595855713\n",
      "156 1.3047046661376953\n",
      "162 0.2558390498161316\n",
      "193 0.25004395842552185\n",
      "209 0.22625890374183655\n",
      "271 0.4073232412338257\n",
      "334 0.2312610149383545\n",
      "378 0.2749529778957367\n",
      "394 0.25855159759521484\n",
      "410 0.24639737606048584\n",
      "473 0.2546446621417999\n",
      "503 0.30264389514923096\n",
      "Component 4:\n",
      "Component 5:\n",
      "Component 6:\n",
      "14 0.32152992486953735\n",
      "56 0.36937186121940613\n",
      "89 0.2253105342388153\n",
      "98 0.45865800976753235\n",
      "111 1.0191481113433838\n",
      "271 0.378071665763855\n",
      "369 0.3085884153842926\n",
      "410 0.38648104667663574\n",
      "503 0.4295293986797333\n",
      "Component 7:\n",
      "Component 8:\n",
      "Component 9:\n",
      "23 0.22006076574325562\n",
      "47 0.4983331859111786\n",
      "89 0.24302326142787933\n",
      "128 0.22135122120380402\n",
      "186 0.3102020025253296\n",
      "263 0.26080626249313354\n",
      "367 0.23302249610424042\n",
      "390 0.26373472809791565\n",
      "416 0.2303304672241211\n",
      "457 0.21987216174602509\n",
      "Component 10:\n",
      "Component 11:\n",
      "Component 12:\n",
      "488 0.2371566742658615\n",
      "Component 13:\n",
      "Component 14:\n",
      "Component 15:\n",
      "total neurons: 52\n"
     ]
    }
   ],
   "source": [
    "neurons_to_ablate = {}\n",
    "total_neurons = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    neurons_to_ablate[submodules[component_idx]] = []\n",
    "    for idx in (effect.act > 0.2135).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        neurons_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_neurons += 1\n",
    "print(f\"total neurons: {total_neurons}\")\n",
    "\n",
    "neurons_to_ablate = {\n",
    "    submodule : n_hot([neuron_idx], dim=512) for submodule, neuron_idx in neurons_to_ablate.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts_abl(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x[...,neurons_to_ablate[submodule]] = x.mean(dim=(0,1))[...,neurons_to_ablate[submodule]] # mean ablation\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = x\n",
    "            else:\n",
    "                submodule.output = x\n",
    "\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "    return act.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9889637231826782\n",
      "Ground truth accuracy: 0.7355991005897522\n",
      "Spurious accuracy: 0.7528801560401917\n"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9891391396522522\n",
      "Accuracy for (0, 1): 0.3658280074596405\n",
      "Accuracy for (1, 0): 0.5115207433700562\n",
      "Accuracy for (1, 1): 0.9937267303466797\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline feature performance\n",
    "from paper: Instead of relying on human judgement to evaluate whether a\n",
    "feature should be ablated, we zero-ablate the 55 features from our circuit that are\n",
    "most causally implicated in spurious feature accuracy on the balanced set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:04<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# get features which are most useful for predicting gender label\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if running_nodes is None:\n",
    "            running_nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                running_nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in running_nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "946 0.25006115436553955\n",
      "7392 0.6816262602806091\n",
      "17846 0.2601848244667053\n",
      "28533 0.34427016973495483\n",
      "29476 0.14480449259281158\n",
      "31467 0.3418383300304413\n",
      "32081 0.1740640252828598\n",
      "32469 1.2141445875167847\n",
      "Component 1:\n",
      "Component 2:\n",
      "3842 0.1846996247768402\n",
      "10258 0.2082471251487732\n",
      "13387 0.2748056948184967\n",
      "18382 0.11551540344953537\n",
      "19369 0.11571113020181656\n",
      "28127 0.9595451354980469\n",
      "30518 0.20731867849826813\n",
      "31645 0.2460571974515915\n",
      "Component 3:\n",
      "1022 0.4973563551902771\n",
      "3122 0.20655781030654907\n",
      "9651 0.4463863968849182\n",
      "10060 2.1757450103759766\n",
      "18967 0.9766783118247986\n",
      "22084 0.16898095607757568\n",
      "23898 0.27730074524879456\n",
      "26504 0.1362760365009308\n",
      "29626 0.3603300452232361\n",
      "Component 4:\n",
      "Component 5:\n",
      "24159 0.30897560715675354\n",
      "25018 0.3229933977127075\n",
      "Component 6:\n",
      "4592 0.2170836329460144\n",
      "8920 0.5713692307472229\n",
      "9877 0.2788275182247162\n",
      "12128 0.5513262152671814\n",
      "12436 0.20832858979701996\n",
      "15017 3.22841739654541\n",
      "26204 0.12446071207523346\n",
      "30248 0.6778391599655151\n",
      "Component 7:\n",
      "13570 0.11081066727638245\n",
      "27472 1.0406968593597412\n",
      "Component 8:\n",
      "Component 9:\n",
      "1995 0.8690407872200012\n",
      "9128 1.5634136199951172\n",
      "11656 0.1676454246044159\n",
      "12440 0.1573600023984909\n",
      "29206 0.18726511299610138\n",
      "29295 0.3985458016395569\n",
      "30263 0.12636373937129974\n",
      "Component 10:\n",
      "2959 0.8676270842552185\n",
      "19128 0.28398358821868896\n",
      "22029 0.13728690147399902\n",
      "Component 11:\n",
      "Component 12:\n",
      "19558 1.3080941438674927\n",
      "23545 0.3948901295661926\n",
      "27334 0.1667580008506775\n",
      "Component 13:\n",
      "22821 0.11310113966464996\n",
      "31101 0.49193111062049866\n",
      "Component 14:\n",
      "Component 15:\n",
      "9766 0.17570216953754425\n",
      "12420 1.1634024381637573\n",
      "30220 0.2979702651500702\n",
      "total features: 55\n"
     ]
    }
   ],
   "source": [
    "top_feats_to_ablate = {}\n",
    "total_features = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    top_feats_to_ablate[submodules[component_idx]] = []\n",
    "    for idx in (effect > 0.1107).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        top_feats_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_features += 1\n",
    "print(f\"total features: {total_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats_to_ablate = {\n",
    "    submodule : n_hot(feats) for submodule, feats in top_feats_to_ablate.items()\n",
    "}\n",
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, top_feats_to_ablate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9312267303466797\n",
      "Ground truth accuracy: 0.8894008994102478\n",
      "Spurious accuracy: 0.539170503616333\n"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9735883474349976\n",
      "Accuracy for (0, 1): 0.9285767674446106\n",
      "Accuracy for (1, 0): 0.7718893885612488\n",
      "Accuracy for (1, 1): 0.8894051909446716\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining probe on activations after ablating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9572490453720093\n",
      "Ground truth accuracy: 0.9308755993843079\n",
      "Unintended feature accuracy: 0.5195852518081665\n"
     ]
    }
   ],
   "source": [
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, feats_to_ablate)\n",
    "\n",
    "new_probe, _ = train_probe(get_acts_abl, label_idx=0)\n",
    "print('Ambiguous test accuracy:', test_probe(new_probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison  \n",
    "Probe results:  \n",
    "Ambiguous test accuracy: 0.9955855011940002  \n",
    "Ground truth accuracy: 0.6186636090278625  \n",
    "Unintended feature accuracy: 0.8744239807128906  \n",
    "\n",
    "After ablating features:  \n",
    "Ambiguous test accuracy: 0.926347553730011  \n",
    "Ground truth accuracy: 0.8853686451911926  \n",
    "Spurious accuracy: 0.5397465229034424  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9477938413619995\n",
      "Accuracy for (0, 1): 0.8895981907844543\n",
      "Accuracy for (1, 0): 0.9423962831497192\n",
      "Accuracy for (1, 1): 0.9679368138313293\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: 0 hours, 4 minutes, 31.49 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Convert the execution time to hours, minutes, and seconds\n",
    "hours, remainder = divmod(execution_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(f\"Total execution time: {int(hours)} hours, {int(minutes)} minutes, {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_fc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
