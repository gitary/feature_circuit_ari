{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Libraries + **MODEL** (EleutherAI/pythia-70m-deduped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.dictionary import IdentityDict\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "# model hyperparameters\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "activation_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "+ Ambiguous/balanced set:  \n",
    "    + The ambiguous set, consisting of bios of male professors (labeled 0) and female nurses (labeled 1).  \n",
    "    + The balanced set, consisting of an equal number of bios for male professors, male nurses, female professors, and female nurses.  \n",
    "  \n",
    "+ Train/test  \n",
    "  \n",
    "+ Intended label [profession] (0) / Unintended label [gender] (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset hyperparameters\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "# data preparation hyperparameters\n",
    "batch_size = 1024\n",
    "SEED = 42\n",
    "\n",
    "# To fit on 24GB VRAM GPU, I set the next 2 default batch_sizes to 64\n",
    "def get_data(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    \"\"\"\n",
    "    Loads and processes the dataset for training or testing.\n",
    "\n",
    "    Parameters:\n",
    "    train (bool): If True, loads the training set; otherwise, loads the test set. Default is True.\n",
    "    ambiguous (bool): If True, loads the ambiguous set; otherwise, loads the balanced set. Default is True.\n",
    "    batch_size (int): The size of each batch. Default is 128.\n",
    "    seed (int): The random seed for shuffling the data. Default is SEED.\n",
    "\n",
    "    Returns:\n",
    "    list of tuples: A list of batches, where each batch is a tuple containing:\n",
    "        - data (list): A list of text data (bios).\n",
    "        - true_labels (torch.Tensor): A tensor of intended labels (profession).\n",
    "        - spurious_labels (torch.Tensor): A tensor of unintended labels (gender).\n",
    "    \"\"\"\n",
    "    #• The ambiguous set, consisting of bios of male professors (labeled 0) and female nurses (labeled 1).\n",
    "    #• The balanced set, consisting of an equal number of bios for male professors, male nurses, female professors, and female nurses.\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    \"\"\"\n",
    "    Generates subgroups of text data based on gender and profession labels.\n",
    "    Parameters:\n",
    "    train (bool): If True, use training data; otherwise, use test data. Default is True.\n",
    "    ambiguous (bool): If True, create two subgroups (ambiguous); otherwise, create four subgroups (non-ambiguous). Default is True.\n",
    "    batch_size (int): The size of each batch of data. Default is 128.\n",
    "    seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are label profiles (tuples) and values are lists of batches. Each batch is a tuple containing:\n",
    "        - A list of text data.\n",
    "        - A tensor of the first label repeated for the batch size.\n",
    "        - A tensor of the second label repeated for the batch size.\n",
    "    \"\"\"\n",
    "\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_labels, pos_labels = (0, 0), (1, 1)\n",
    "        subgroups = [(neg, neg_labels), (pos, pos_labels)]\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "        subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "    \n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probe - Linear classifier\n",
    "Train and Test  \n",
    "+ get activation form the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe training hyperparameters\n",
    "\n",
    "layer = 4 # model layer for attaching linear classification head (second-last layer)\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to(DEVICE)\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1] \n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses\n",
    "\n",
    "def test_probe(probe, get_acts, label_idx=0, batches=get_data(train=False), seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.0).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()\n",
    "    \n",
    "def get_acts(text):\n",
    "    with t.no_grad(): \n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            attn_mask = model.input[1]['attention_mask']\n",
    "            acts = model.gpt_neox.layers[layer].output[0]\n",
    "            acts = acts * attn_mask[:, :, None]\n",
    "            acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_test = get_acts(bb[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.4385e-01, -1.3381e-01, -1.4041e-01,  ..., -1.0235e-02,\n",
       "           1.2699e+00, -2.4859e-01],\n",
       "         [ 3.4698e-01, -1.0725e-01, -6.2551e-02,  ...,  2.1407e-02,\n",
       "           1.2962e+00, -2.9423e-01],\n",
       "         [ 3.6668e-01, -1.2669e-01, -1.2748e-01,  ...,  8.6772e-02,\n",
       "           1.2887e+00, -3.3930e-01],\n",
       "         ...,\n",
       "         [ 2.9722e-03, -4.7632e-01, -3.1562e-01,  ...,  8.1343e-01,\n",
       "           1.5681e+00, -4.6374e-01],\n",
       "         [ 2.2903e-01, -8.1281e-01, -2.3526e-01,  ...,  2.1779e-01,\n",
       "           4.3966e-01, -7.1307e-02],\n",
       "         [ 5.2845e-01, -3.1384e-01,  1.1301e-01,  ...,  5.5013e-01,\n",
       "           1.0357e+00,  2.1852e-01]],\n",
       "\n",
       "        [[ 3.9677e-01, -2.4028e-01, -3.9316e-01,  ...,  2.0073e-01,\n",
       "           1.0912e+00, -7.6334e-01],\n",
       "         [ 4.7017e-01, -2.2216e-01, -3.9180e-01,  ...,  1.7163e-01,\n",
       "           1.0646e+00, -7.6113e-01],\n",
       "         [ 4.5083e-01, -2.2815e-01, -3.4973e-01,  ...,  1.8556e-01,\n",
       "           9.8221e-01, -7.8530e-01],\n",
       "         ...,\n",
       "         [ 4.7234e-02, -6.4040e-01,  1.1958e-01,  ..., -5.0122e-01,\n",
       "           9.1683e-01, -3.2351e-01],\n",
       "         [-1.1829e-01, -4.6244e-01,  5.4738e-02,  ..., -1.8263e-01,\n",
       "           5.5737e-01, -3.7452e-02],\n",
       "         [ 9.1169e-02,  6.8209e-02,  5.4482e-01,  ..., -1.5869e-01,\n",
       "           1.7029e+00, -4.6073e-01]],\n",
       "\n",
       "        [[ 1.0094e-01, -2.8126e-01, -1.3971e-02,  ...,  4.1550e-01,\n",
       "           1.0479e+00, -4.2472e-01],\n",
       "         [ 9.9583e-02, -2.6832e-01, -8.2369e-03,  ...,  3.8522e-01,\n",
       "           1.0833e+00, -3.8802e-01],\n",
       "         [ 1.3864e-01, -2.7211e-01,  1.0275e-01,  ...,  3.6105e-01,\n",
       "           1.1448e+00, -3.4672e-01],\n",
       "         ...,\n",
       "         [-8.0221e-01,  7.1779e-01, -2.2723e-01,  ..., -7.8321e-02,\n",
       "           1.7077e+00,  2.5143e-01],\n",
       "         [-4.1222e-01, -1.2963e-01,  6.2510e-01,  ...,  5.2959e-01,\n",
       "           1.8487e+00, -1.0073e+00],\n",
       "         [ 2.2163e-01,  6.2903e-02, -2.4636e-01,  ...,  2.9155e-01,\n",
       "           1.4291e+00, -1.1033e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.0255e-01, -3.4836e-01, -3.2832e-01,  ...,  1.6456e-01,\n",
       "           1.1804e+00, -7.2510e-01],\n",
       "         [ 4.7987e-01, -2.9920e-01, -3.4925e-01,  ...,  2.5643e-01,\n",
       "           1.1595e+00, -7.0842e-01],\n",
       "         [ 5.1226e-01, -2.3717e-01, -3.3315e-01,  ...,  2.9421e-01,\n",
       "           1.0864e+00, -7.2104e-01],\n",
       "         ...,\n",
       "         [ 2.9363e-02, -5.4618e-01,  6.1016e-02,  ..., -4.6990e-01,\n",
       "           1.0321e+00, -1.8824e-01],\n",
       "         [-1.2294e-01, -6.1783e-01, -7.8672e-03,  ..., -2.4745e-01,\n",
       "           7.2140e-01, -5.1229e-02],\n",
       "         [ 8.3215e-02,  3.3517e-01,  1.8117e-01,  ..., -1.6795e-01,\n",
       "           1.6601e+00, -2.9153e-01]],\n",
       "\n",
       "        [[ 3.8974e-01, -2.7884e-01, -3.8225e-01,  ...,  1.7353e-01,\n",
       "           1.0887e+00, -6.0204e-01],\n",
       "         [ 4.3676e-01, -2.6383e-01, -4.2019e-01,  ...,  1.8474e-01,\n",
       "           1.1166e+00, -6.8475e-01],\n",
       "         [ 5.0301e-01, -1.9584e-01, -4.1308e-01,  ...,  2.7955e-01,\n",
       "           1.0267e+00, -7.1950e-01],\n",
       "         ...,\n",
       "         [ 3.5203e-01, -6.7136e-01,  1.6862e-02,  ..., -3.3950e-01,\n",
       "           7.6077e-01, -3.7473e-01],\n",
       "         [-4.5215e-02, -4.8650e-01,  8.7265e-02,  ..., -2.5519e-01,\n",
       "           7.1567e-01, -4.3121e-02],\n",
       "         [ 4.6113e-01,  3.5103e-01,  5.8316e-01,  ..., -2.1318e-01,\n",
       "           1.4562e+00, -4.2385e-01]],\n",
       "\n",
       "        [[ 4.7300e-01, -2.6067e-01, -3.8476e-01,  ...,  1.1028e-01,\n",
       "           9.7362e-01, -7.6941e-01],\n",
       "         [ 5.3551e-01, -2.2772e-01, -3.8472e-01,  ...,  1.6381e-01,\n",
       "           9.6251e-01, -7.8864e-01],\n",
       "         [ 5.4098e-01, -1.7178e-01, -3.5944e-01,  ...,  1.9447e-01,\n",
       "           9.1685e-01, -7.8460e-01],\n",
       "         ...,\n",
       "         [-1.2532e-02, -7.0564e-01,  3.7792e-02,  ..., -5.2069e-01,\n",
       "           8.7987e-01, -2.2822e-01],\n",
       "         [-1.6222e-01, -6.2183e-01, -1.5562e-03,  ..., -2.7133e-01,\n",
       "           6.4278e-01, -1.3150e-01],\n",
       "         [ 2.9706e-01,  4.2904e-01,  2.6977e-01,  ..., -4.3594e-01,\n",
       "           9.2517e-01, -7.1334e-01]]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_acts_test(text):\n",
    "    with t.no_grad(): \n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            #attn_mask = model.input[1]['attention_mask']\n",
    "            print('ciao')\n",
    "            acts = model.gpt_neox.layers[layer].output[0]\n",
    "            #acts = acts * attn_mask[:, :, None]\n",
    "            #acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value\n",
    "    \n",
    "\n",
    "get_acts_test(bb[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nnsight.models.LanguageModel.LanguageModelProxy'>\n"
     ]
    }
   ],
   "source": [
    "def get_acts_test(text):\n",
    "    with t.no_grad(): \n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            #attn_mask = model.input[1]['attention_mask']\n",
    "            acts = model.gpt_neox.layers[layer].output[0]\n",
    "            print(type(acts))\n",
    "            #acts = acts * attn_mask[:, :, None]\n",
    "            #acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value\n",
    "    \n",
    "ttt = get_acts_test(bb[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 212, 512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle  \n",
    "A classifier trained on ground-truth labels on the <u>**balanced**</u> set  \n",
    "Intended (ground-truth) labels [profession] (label_idx=0) / Unintended label [gender] (label_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous test accuracy 0.9318076372146606\n",
      "ground truth accuracy: 0.9302995204925537\n",
      "unintended feature accuracy: 0.49366357922554016\n"
     ]
    }
   ],
   "source": [
    "oracle, _ = train_probe(get_acts, label_idx=0, batches=get_data(ambiguous=False)) # train on balanced training set\n",
    "print(\"ambiguous test accuracy\", test_probe(oracle, get_acts, label_idx=0)) # test on ambiguous test set\n",
    "batches = get_data(train=False, ambiguous=False) # balanced test set\n",
    "print(\"ground truth accuracy:\", test_probe(oracle, get_acts, batches=batches, label_idx=0))\n",
    "print(\"unintended feature accuracy:\", test_probe(oracle, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9468682408332825\n",
      "Accuracy for (0, 1): 0.926398754119873\n",
      "Accuracy for (1, 0): 0.9239631295204163\n",
      "Accuracy for (1, 1): 0.9189126491546631\n"
     ]
    }
   ],
   "source": [
    "# get worst-group accuracy of oracle probe\n",
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "# label_profile: (profession (professor/nurse), gender (male/female)) [but I don't know in what order]\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(oracle, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probe\n",
    "A classifier trained on ground-truth labels on the <u>**ambiguous**</u> set  \n",
    "Intended (ground-truth) labels [profession] (label_idx=0) / Unintended label [gender] (label_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9955855011940002\n",
      "Ground truth accuracy: 0.6186636090278625\n",
      "Unintended feature accuracy: 0.8744239807128906\n"
     ]
    }
   ],
   "source": [
    "probe, _ = train_probe(get_acts, label_idx=0)\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9978401064872742\n",
      "Accuracy for (0, 1): 0.24355988204479218\n",
      "Accuracy for (1, 0): 0.2626728117465973\n",
      "Accuracy for (1, 1): 0.9934943914413452\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary - SAEs + metric_fn\n",
    "<img src=\"../images/LLM_schema.png\" alt=\"LLM_schema\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of components, that is num of SAEs: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of components, that is num of SAEs:\", 1+(layer + 1)*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dictionaries\n",
    "\n",
    "# dictionary hyperparameters\n",
    "dict_id = 10\n",
    "expansion_factor = 64\n",
    "dictionary_size = expansion_factor * activation_dim\n",
    "\n",
    "submodules = []\n",
    "dictionaries = {}\n",
    "\n",
    "submodules.append(model.gpt_neox.embed_in)\n",
    "dictionaries[model.gpt_neox.embed_in] = AutoEncoder.from_pretrained(\n",
    "    f'../dictionaries/pythia-70m-deduped/embed/{dict_id}_{dictionary_size}/ae.pt',\n",
    "    device=DEVICE\n",
    ")\n",
    "for i in range(layer + 1):\n",
    "    submodules.append(model.gpt_neox.layers[i].attention)\n",
    "    dictionaries[model.gpt_neox.layers[i].attention] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/attn_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i].mlp)\n",
    "    dictionaries[model.gpt_neox.layers[i].mlp] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/mlp_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i])\n",
    "    dictionaries[model.gpt_neox.layers[i]] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/resid_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "#probe is the linear classifier trained on ambiguous training set\n",
    "def metric_fn(model, labels=None):\n",
    "    attn_mask = model.input[1]['attention_mask']\n",
    "    acts = model.gpt_neox.layers[layer].output[0]\n",
    "    acts = acts * attn_mask[:, :, None]\n",
    "    acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "    \n",
    "    return t.where(\n",
    "        labels == 0,\n",
    "        probe(acts),\n",
    "        - probe(acts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most influential features\n",
    "Patching effect  \n",
    "ig = integrated gradient  \n",
    "  \n",
    "+ <u>Attribution patching</u>: activation patching at industrial scale, it uses gradients to take linear approximation to activation patching\n",
    "+ <u>**Integrated gradients**</u>: a more expensive but more accurate approximation (applicable since we use small models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:23<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# find most influential features\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, labels, _) in tqdm(enumerate(get_data(train=True, ambiguous=True, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold\n",
    "We apply some choice of node\n",
    "threshold TN to select nodes with a large (absolute) IE.  \n",
    "To keep the number of nodes we need to annotate manageable, we set a relatively high\n",
    "node threshold of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "946 0.22117570042610168\n",
      "5719 0.14886397123336792\n",
      "7392 0.3218412399291992\n",
      "10784 0.15049311518669128\n",
      "17846 0.31437844038009644\n",
      "22068 0.1622835248708725\n",
      "23079 0.15247352421283722\n",
      "25904 0.10423339903354645\n",
      "28533 0.18998156487941742\n",
      "29476 0.19172973930835724\n",
      "31461 0.16866040229797363\n",
      "31467 0.16690056025981903\n",
      "32081 0.32751068472862244\n",
      "32469 1.3901286125183105\n",
      "Component 1:\n",
      "23752 0.1079249158501625\n",
      "Component 2:\n",
      "2995 0.10343753546476364\n",
      "3842 0.14583298563957214\n",
      "10258 0.29746800661087036\n",
      "13387 0.14572963118553162\n",
      "13968 0.12934662401676178\n",
      "18382 0.2589782476425171\n",
      "19369 0.18766063451766968\n",
      "28127 1.0541187524795532\n",
      "30518 0.19712230563163757\n",
      "Component 3:\n",
      "1022 0.31592071056365967\n",
      "9651 0.5887966156005859\n",
      "10060 2.43733549118042\n",
      "18967 0.6829056143760681\n",
      "22084 0.2536494731903076\n",
      "23898 0.4724956154823303\n",
      "24799 0.10279475897550583\n",
      "26504 0.30445045232772827\n",
      "29626 0.2844652533531189\n",
      "31201 0.17182287573814392\n",
      "Component 4:\n",
      "8147 0.10606715828180313\n",
      "Component 5:\n",
      "24159 0.24803251028060913\n",
      "25018 0.4943896234035492\n",
      "Component 6:\n",
      "4592 0.3920895755290985\n",
      "8920 0.5931018590927124\n",
      "9877 0.3653258681297302\n",
      "12128 0.6474209427833557\n",
      "15017 3.1611530780792236\n",
      "17369 0.10423677414655685\n",
      "26969 0.10391923785209656\n",
      "30248 1.0404223203659058\n",
      "Component 7:\n",
      "13570 0.1257641464471817\n",
      "27472 1.238729476928711\n",
      "Component 8:\n",
      "Component 9:\n",
      "1995 1.0039465427398682\n",
      "9128 1.5338925123214722\n",
      "11656 0.1635110080242157\n",
      "12440 0.18845383822917938\n",
      "14638 0.15359634160995483\n",
      "29206 0.21679852902889252\n",
      "29295 0.6558160781860352\n",
      "31098 0.11044643074274063\n",
      "Component 10:\n",
      "2959 0.9745623469352722\n",
      "19128 0.26585495471954346\n",
      "22029 0.17818133533000946\n",
      "Component 11:\n",
      "Component 12:\n",
      "19558 1.547678828239441\n",
      "23545 0.3753567934036255\n",
      "24806 0.12211156636476517\n",
      "27334 0.15601801872253418\n",
      "31453 0.17198815941810608\n",
      "Component 13:\n",
      "31101 0.5748973488807678\n",
      "Component 14:\n",
      "Component 15:\n",
      "9766 0.23256155848503113\n",
      "12420 1.3541486263275146\n",
      "30220 0.2663297951221466\n",
      "total features: 67\n"
     ]
    }
   ],
   "source": [
    "n_features = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    for idx in (effect > 0.1).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        n_features += 1\n",
    "print(f\"total features: {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANUALLY Examine most influential features  \n",
    "Manually inspect and evaluate for task-relevancy each feature in the circuit from\n",
    "previous Step. For each feature, examine the text data that activates the feature the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bde26148dc1438d8eb6b12803229478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/feature_circuit_ari/env_fc/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' nursing', 5.101802825927734), (' Nursing', 3.9356651306152344), (' nurse', 2.8416316509246826), (' nurses', 2.8025169372558594), (' RN', 1.5293744802474976), (' Teaching', 0.7016171216964722), (' rehabilitation', 0.6920690536499023), ('wife', 0.6749840974807739), ('unte', 0.6074658632278442), (' sewing', 0.6034237146377563), (' caring', 0.4292561411857605), ('ancy', 0.3882577419281006), ('akers', 0.38148069381713867), (' lending', 0.34468239545822144), (' volunteers', 0.3259945511817932), (' drinking', 0.2667090892791748), (' Clinical', 0.26244115829467773), (' inpatient', 0.23919767141342163), (' architect', 0.22628939151763916), (' Medical', 0.21756377816200256), (' relational', 0.19916176795959473), (' Dou', 0.19640305638313293), (' dialysis', 0.1761435568332672), (' Leadership', 0.1721816062927246), ('hin', 0.14378538727760315), ('okin', 0.1413027048110962), (' executive', 0.11437027156352997), (' teaching', 0.11418899148702621), (' poet', 0.10126781463623047), (' hospitals', 0.0999029278755188)]\n",
      "[('�', 1.2042195796966553), ('��', 1.1853760480880737), ('�', 1.1379964351654053), ('�', 1.1257283687591553), ('�', 1.0499390363693237), (' home', 0.9965490698814392), (' homes', 0.9865149855613708), ('�', 0.9813761711120605), ('�', 0.9332275986671448), ('�', 0.9122070670127869), ('�', 0.8883179426193237), ('giving', 0.8878536224365234), ('�', 0.8529338240623474), ('�', 0.8425619006156921), ('�', 0.8414469957351685), ('��', 0.8361857533454895), ('��', 0.8251465559005737), ('�', 0.7906738519668579), ('�', 0.7814779281616211), ('��', 0.7766968011856079), ('�', 0.7744954824447632), ('�', 0.7614461779594421), ('��', 0.7600098252296448), ('�', 0.7405151724815369), ('�', 0.7403483390808105), ('�', 0.7380574941635132), (' practitioner', 0.7281612157821655), ('�', 0.7212402820587158), ('��', 0.7185587882995605), ('�', 0.7153890132904053)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a66eab30-1398\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a66eab30-1398\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\", \"\\n\", \"\\n\", \"Dr\", \".\", \" Gregory\", \" has\", \" Doctor\", \"ate\", \" of\", \" Nursing\", \" Practice\", \" in\", \" Health\", \" Innovation\", \" and\", \" Leadership\", \" from\", \" the\", \" University\", \" of\", \" Minnesota\", \" and\", \" a\", \" b\", \"achel\", \"ors\", \" in\", \" nursing\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\"], [\"A\", \" small\", \" city\", \" in\", \" Iowa\", \" has\", \" taken\", \" an\", \" action\", \" to\", \" save\", \" the\", \" bees\", \" from\", \" extinction\", \".\", \" Ac\", \"res\", \" of\", \" land\", \" were\", \" donated\", \" to\", \" increase\", \" the\", \" local\", \" habitats\", \" of\", \" the\", \" bees\", \".\", \"\\n\", \"\\n\", \"Over\", \" the\", \" past\", \" decade\", \",\", \" bees\", \" are\", \" steadily\", \" disappearing\", \".\", \" Work\", \"er\", \" bees\", \" disappear\", \" and\", \" leaving\", \" behind\", \" the\", \" queen\", \".\", \" With\", \" a\", \" few\", \" nursing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\", \"\\n\", \"\\n\", \"Dr\", \".\", \" Gregory\", \" has\", \" Doctor\", \"ate\", \" of\", \" Nursing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\", \" provision\", \" for\", \" nurses\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\"], [\"No\", \" other\", \" appliance\", \" company\", \" has\", \" a\", \" wider\", \" scope\", \" of\", \" solutions\", \",\", \" nor\", \" the\", \" experience\", \" to\", \" back\", \" them\", \" up\", \",\", \" than\", \" Elect\", \"rol\", \"ux\", \".\", \" Our\", \" long\", \" presence\", \" in\", \" people\", \"\\u2019\", \"s\", \" homes\", \" around\", \" the\", \" world\", \" means\", \" that\", \" no\", \" other\", \" appliance\", \" company\", \" ...\", \"\\n\", \"Read\", \" more\", \"\\n\", \"\\n\", \"...\", \" Easy\", \"-\", \"F\", \"lo\", \" vacu\", \"ums\", \" including\", \" parts\", \" and\", \" bags\", \".\", \" Find\", \"lay\", \"'s\", \" also\", \" offers\", \" sales\", \" and\", \" service\", \" for\", \" all\", \" makes\", \" and\", \" models\", \" of\", \" sewing\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\", \"-\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\"], [\"PCI\", \" Alternative\", \" Using\", \" S\", \"ust\", \"ained\", \" Exercise\", \" (\", \"PA\", \"USE\", \"):\", \" R\", \"ational\", \"e\", \" and\", \" trial\", \" design\", \".\", \"\\n\", \"Card\", \"i\", \"ovascular\", \" disease\", \" (\", \"C\", \"VD\", \")\", \" currently\", \" claims\", \" nearly\", \" one\", \" million\", \" lives\", \" yearly\", \" in\", \" the\", \" US\", \",\", \" accounting\", \" for\", \" nearly\", \" 40\", \"%\", \" of\", \" all\", \" deaths\", \".\", \" Coron\", \"ary\", \" artery\", \" disease\", \" (\", \"CAD\", \")\", \" accounts\", \" for\", \" the\", \" largest\", \" number\", \" of\", \" these\", \" deaths\", \".\", \" While\", \" efforts\", \" aimed\", \" at\", \" treating\", \" CAD\", \" in\", \" recent\", \" decades\", \" have\", \" concentrated\", \" on\", \" surgical\", \" and\", \" catheter\", \"-\", \"based\", \" interventions\", \",\", \" limited\", \" resources\", \" have\", \" been\", \" directed\", \" toward\", \" prevention\", \" and\", \" rehabilitation\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\", \"-\", \"mid\", \"wife\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\"], [\"Account\", \"ing\", \"\\n\", \"\\n\", \"Sur\", \"f\", \" Works\", \" offer\", \" a\", \" range\", \" of\", \" accounting\", \" services\", \" suitable\", \" for\", \" all\", \" types\", \" of\", \" business\", \".\", \" Below\", \",\", \" we\", \" have\", \" listed\", \" packages\", \" suitable\", \" for\", \" sole\", \" traders\", \",\", \" partnerships\", \" and\", \" limited\", \" companies\", \".\", \" The\", \" packages\", \" can\", \" be\", \" fully\", \" tailored\", \" to\", \" your\", \" requirements\", \" by\", \" adding\", \" extra\", \" services\", \" to\", \" create\", \" the\", \" exact\", \" service\", \" that\", \" you\", \" and\", \" your\", \" business\", \" requires\", \".\", \"\\n\", \"\\n\", \"All\", \" services\", \" are\", \" carried\", \" out\", \" on\", \" time\", \" with\", \" the\", \" minimum\", \" of\", \" fuss\", \" by\", \" our\", \" in\", \" house\", \",\", \" fully\", \" qualified\", \" accountant\", \"\\n\", \"\\n\", \"The\", \" list\", \" of\", \" services\", \" offered\", \" is\", \" not\", \" exhaustive\", \" so\", \" please\", \" let\", \" us\", \" know\", \" if\", \" you\", \" require\", \" a\", \" service\", \" not\", \" listed\", \".\", \" If\", \" you\", \" have\", \" specific\", \" needs\", \" we\", \" can\", \" build\", \" a\", \" bes\", \"p\", \"oke\", \" account\", \"ancy\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\", \" provision\", \" for\", \" nurses\", \" and\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"ers\", \" at\", \" CAM\", \"C\", \" bring\", \" their\", \" unique\", \" personalities\", \" and\", \" skills\", \" to\", \" our\", \" hospital\", \".\", \" They\", \" range\", \" in\", \" age\", \" from\", \" 15\", \" to\", \" 99\", \".\", \" Our\", \" ranks\", \" are\", \" made\", \" up\", \" of\", \" men\", \" and\", \" women\", \";\", \" students\", \" and\", \" retire\", \"es\", \";\", \" homem\", \"akers\"], [\"Got\", \" this\", \" cute\", \" little\", \" sewing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\", \"\\n\", \"\\n\", \"Dr\", \".\", \" Gregory\", \" has\", \" Doctor\", \"ate\", \" of\", \" Nursing\", \" Practice\", \" in\", \" Health\", \" Innovation\", \" and\", \" Leadership\", \" from\", \" the\", \" University\", \" of\", \" Minnesota\", \" and\", \" a\", \" b\", \"achel\", \"ors\"], [\"Micro\", \"-\", \"Lo\", \"an\", \" Program\", \"\\n\", \"\\n\", \"In\", \" order\", \" to\", \" promote\", \" economic\", \" development\", \" in\", \" the\", \" City\", \" of\", \" Al\", \"amo\", \",\", \" the\", \" Al\", \"amo\", \" ED\", \"C\", \" established\", \" the\", \" Al\", \"amo\", \" Small\", \" Business\", \" Micro\", \"-\", \"Lo\", \"an\", \" Program\", \" (\", \"ML\", \"P\", \")\", \" with\", \" assistance\", \" from\", \" USDA\", \" \\u2013\", \" Rural\", \" Development\", \".\", \" The\", \" M\", \"LP\", \" is\", \" a\", \" self\", \"-\", \"s\", \"ust\", \"aining\", \" project\", \" that\", \" works\", \" by\", \" lending\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\"], [\"[\", \"Central\", \" venous\", \" dialysis\", \" catheter\", \".\", \" Sil\", \"ic\", \"one\", \" rubber\", \" dialysis\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937150478363037]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431107997894287]], [[0.0]], [[0.0]], [[0.010102808475494385]], [[0.13302087783813477]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02793020009994507]], [[0.0]], [[3.9356651306152344]], [[0.0]], [[0.0]], [[0.0]], [[0.061070144176483154]], [[0.0]], [[0.1721816062927246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.35008299350738525]], [[0.0]], [[5.229028701782227]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174833297729492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.97146463394165]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.701827049255371]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937150478363037]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431107997894287]], [[0.0]], [[0.0]], [[0.010102808475494385]], [[0.13302087783813477]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02793020009994507]], [[0.0]], [[3.9356651306152344]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937150478363037]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.021908700466156006]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8095011711120605]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174833297729492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.97146463394165]], [[0.3862197995185852]], [[0.16560733318328857]], [[0.0]], [[0.0]], [[0.020936012268066406]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34256696701049805]], [[0.0]], [[0.0]], [[2.8025169372558594]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351275563240051]], [[0.0]], [[0.43192535638809204]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4292561411857605]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520798683166504]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8406283855438232]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.021908700466156006]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8095011711120605]], [[0.7401372194290161]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6920690536499023]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.021908700466156006]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8095011711120605]], [[0.7401372194290161]], [[0.0]], [[0.6749840974807739]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351275563240051]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351275563240051]], [[0.0]], [[0.43192535638809204]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351275563240051]], [[0.0]], [[0.43192535638809204]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4292561411857605]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3882577419281006]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174833297729492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.97146463394165]], [[0.3862197995185852]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174833297729492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.97146463394165]], [[0.3862197995185852]], [[0.16560733318328857]], [[0.0]], [[0.0]], [[0.020936012268066406]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34256696701049805]], [[0.0]], [[0.0]], [[2.8025169372558594]], [[0.38558393716812134]]], [[[0.0]], [[0.23981177806854248]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351275563240051]], [[0.0]], [[0.43192535638809204]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4292561411857605]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520798683166504]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.38148069381713867]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.36621904373168945]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937150478363037]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431107997894287]], [[0.0]], [[0.0]], [[0.010102808475494385]], [[0.13302087783813477]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02793020009994507]], [[0.0]], [[3.9356651306152344]], [[0.0]], [[0.0]], [[0.0]], [[0.061070144176483154]], [[0.0]], [[0.1721816062927246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.35008299350738525]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34468239545822144]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293744802474976]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937150478363037]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431107997894287]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796510696411133]], [[0.08860629796981812]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174833297729492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.97146463394165]], [[0.3862197995185852]], [[0.16560733318328857]], [[0.0]], [[0.0]], [[0.020936012268066406]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34256696701049805]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.319751501083374]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f44024cebd0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interpret features\n",
    "\n",
    "# change the following two lines to pick which feature to interpret\n",
    "component_idx = 9\n",
    "feat_idx = 31098\n",
    "\n",
    "submodule = submodules[component_idx]\n",
    "dictionary = dictionaries[submodule]\n",
    "\n",
    "# interpret some features\n",
    "data = hf_dataset_to_generator(\"monology/pile-uncopyrighted\")\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    d_submodule=512,\n",
    "    refresh_batch_size=128, # decrease to fit on smaller GPUs\n",
    "    n_ctxs=512, # decrease to fit on smaller GPUs\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "out = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary,\n",
    "    dim_idx=feat_idx,\n",
    "    n_inputs=256 # decrease to fit on smaller GPUs\n",
    ")\n",
    "print(out.top_tokens)\n",
    "print(out.top_affected)\n",
    "out.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features to ablate: 55\n"
     ]
    }
   ],
   "source": [
    "feats_to_ablate = {\n",
    "    submodules[0] : [\n",
    "        946, # 'his'\n",
    "        # 5719, # 'research'\n",
    "        7392, # 'He'\n",
    "        # 10784, # 'Nursing'\n",
    "        17846, # 'He'\n",
    "        22068, # 'His'\n",
    "        # 23079, # 'tastes'\n",
    "        # 25904, # 'nursing'\n",
    "        28533, # 'She'\n",
    "        29476, # 'he'\n",
    "        31461, # 'His'\n",
    "        31467, # 'she'\n",
    "        32081, # 'her'\n",
    "        32469, # 'She'\n",
    "    ],\n",
    "    submodules[1] : [\n",
    "        # 23752, # capitalized words, especially pronouns\n",
    "    ],\n",
    "    submodules[2] : [\n",
    "        2995, # 'he'\n",
    "        3842, # 'She'\n",
    "        10258, # female names\n",
    "        13387, # 'she'\n",
    "        13968, # 'He'\n",
    "        18382, # 'her'\n",
    "        19369, # 'His'\n",
    "        28127, # 'She'\n",
    "        30518, # 'He'\n",
    "    ],\n",
    "    submodules[3] : [\n",
    "        1022, # 'she'\n",
    "        9651, # female names\n",
    "        10060, # 'She'\n",
    "        18967, # 'He'\n",
    "        22084, # 'he'\n",
    "        23898, # 'His'\n",
    "        # 24799, # promotes surnames\n",
    "        26504, # 'her'\n",
    "        29626, # 'his'\n",
    "        # 31201, # 'nursing'\n",
    "    ],\n",
    "    submodules[4] : [\n",
    "        # 8147, # unclear, something with names\n",
    "    ],\n",
    "    submodules[5] : [\n",
    "        24159, # 'She', 'she'\n",
    "        25018, # female names\n",
    "    ],\n",
    "    submodules[6] : [\n",
    "        4592, # 'her'\n",
    "        8920, # 'he'\n",
    "        9877, # female names\n",
    "        12128, # 'his'\n",
    "        15017, # 'she'\n",
    "        # 17369, # contact info\n",
    "        # 26969, # related to nursing\n",
    "        30248, # female names\n",
    "    ],\n",
    "    submodules[7] : [\n",
    "        13570, # promotes male-related words\n",
    "        27472, # female names, promotes female-related words\n",
    "    ],\n",
    "    submodules[8] : [\n",
    "    ],\n",
    "    submodules[9] : [\n",
    "        1995, # promotes female-associated words\n",
    "        9128, # feminine pronouns\n",
    "        11656, # promotes male-associated words\n",
    "        12440, # promotes female-associated words\n",
    "        # 14638, # related to contact information?\n",
    "        29206, # gendered pronouns\n",
    "        29295, # female names\n",
    "        # 31098, # nursing-related words\n",
    "    ],\n",
    "    submodules[10] : [\n",
    "        2959, # promotes female-associated words\n",
    "        19128, # promotes male-associated words\n",
    "        22029, # promotes female-associated words\n",
    "    ],\n",
    "    submodules[11] : [\n",
    "    ],\n",
    "    submodules[12] : [\n",
    "        19558, # promotes female-associated words\n",
    "        23545, # 'she'\n",
    "        24806, # 'her'\n",
    "        27334, # promotes male-associated words\n",
    "        31453, # female names\n",
    "    ],\n",
    "    submodules[13] : [\n",
    "        31101, # promotes female-associated words\n",
    "    ],\n",
    "    submodules[14] : [\n",
    "    ],\n",
    "    submodules[15] : [\n",
    "        9766, # promotes female-associated words\n",
    "        12420, # promotes female pronouns\n",
    "        30220, # promotes male pronouns\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Number of features to ablate: {sum(len(v) for v in feats_to_ablate.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting feats_to_ablate in a more useful format\n",
    "def n_hot(feats, dim=dictionary_size):\n",
    "    out = t.zeros(dim, dtype=t.bool, device=DEVICE)\n",
    "    for feat in feats:\n",
    "        out[feat] = True\n",
    "    return out\n",
    "\n",
    "feats_to_ablate = {\n",
    "    submodule : n_hot(feats) for submodule, feats in feats_to_ablate.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for ablating features\n",
    "is_tuple = {}\n",
    "with t.no_grad(), model.trace(\"_\"):\n",
    "    for submodule in submodules:\n",
    "        is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "def get_acts_ablated(\n",
    "    text,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate\n",
    "):\n",
    "\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            feat_idxs = to_ablate[submodule]\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x_hat, f = dictionary(x, output_features=True)\n",
    "            res = x - x_hat\n",
    "            f[...,feat_idxs] = 0. # zero ablation\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = dictionary.decode(f) + res\n",
    "            else:\n",
    "                submodule.output = dictionary.decode(f) + res\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "    return act.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy after ablating features judged irrelevant by human annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.926347553730011\n",
      "Ground truth accuracy: 0.8853686451911926\n",
      "Spurious accuracy: 0.5397465229034424\n"
     ]
    }
   ],
   "source": [
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, feats_to_ablate)\n",
    "\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison  \n",
    "Probe results:  \n",
    "Ambiguous test accuracy: 0.9955855011940002  \n",
    "Ground truth accuracy: 0.6186636090278625  \n",
    "Unintended feature accuracy: 0.8744239807128906 \n",
    "\n",
    "After ablating features:\n",
    "Ambiguous test accuracy: 0.926347553730011\n",
    "Ground truth accuracy: 0.8853686451911926\n",
    "Spurious accuracy: 0.5397465229034424\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9675408601760864\n",
      "Accuracy for (0, 1): 0.9309049844741821\n",
      "Accuracy for (1, 0): 0.7603686451911926\n",
      "Accuracy for (1, 1): 0.8849906921386719\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept bottleneck probing baseline\n",
    "from paper:  \n",
    "Concept Bottleneck Probing (CBP), adapted from Yan et al. (2023) (originally for\n",
    "multimodal text/image models). CBP works by training a probe to classify inputs\n",
    "x given access only to a vector of affinities between the LM’s representation of x\n",
    "and various concept vectors. See App. E.2 for implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = [    \n",
    "    ' nurse',\n",
    "    ' healthcare',\n",
    "    ' hospital',\n",
    "    ' patient',\n",
    "    ' medical',\n",
    "    ' clinic',\n",
    "    ' triage',\n",
    "    ' medication',\n",
    "    ' emergency',\n",
    "    ' surgery',\n",
    "    ' professor',\n",
    "    ' academia',\n",
    "    ' research',\n",
    "    ' university',\n",
    "    ' tenure',\n",
    "    ' faculty',\n",
    "    ' dissertation',\n",
    "    ' sabbatical',\n",
    "    ' publication',\n",
    "    ' grant',\n",
    "]\n",
    "# get concept vectors\n",
    "with t.no_grad(), model.trace(concepts):\n",
    "    concept_vectors = model.gpt_neox.layers[layer].output[0][:, -1, :].save()\n",
    "concept_vectors = concept_vectors.value - concept_vectors.value.mean(0, keepdim=True)\n",
    "\n",
    "def get_bottleneck(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        acts = model.gpt_neox.layers[layer].output[0]\n",
    "        acts = acts * attn_mask[:, :, None]\n",
    "        acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        # compute cosine similarity with concept vectors\n",
    "        sims = (acts @ concept_vectors.T) / (acts.norm(dim=-1)[:, None] @ concept_vectors.norm(dim=-1)[None])\n",
    "        sims = sims.save()\n",
    "    return sims.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth accuracy: 0.8335253596305847\n",
      "Unintended feature accuracy: 0.600806474685669\n"
     ]
    }
   ],
   "source": [
    "cbp_probe, _ = train_probe(get_bottleneck, label_idx=0, dim=len(concepts))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9276148676872253\n",
      "Accuracy for (0, 1): 0.7864062786102295\n",
      "Accuracy for (1, 0): 0.6774193644523621\n",
      "Accuracy for (1, 1): 0.9409851431846619\n"
     ]
    }
   ],
   "source": [
    "# get subgroup accuracies\n",
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline neuron performance\n",
    "The same as the feature skyline (below) but on neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:10<00:00,  2.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# get neurons which are most influential for giving gender label\n",
    "neuron_dicts = {\n",
    "    submodule : IdentityDict(activation_dim).to(DEVICE) for submodule in submodules\n",
    "}\n",
    "\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        neuron_dicts,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "2 0.26396122574806213\n",
      "42 0.23957575857639313\n",
      "57 0.2323196530342102\n",
      "99 0.32703617215156555\n",
      "130 0.25990810990333557\n",
      "135 0.5472005605697632\n",
      "187 0.3968943655490875\n",
      "197 0.3398488163948059\n",
      "256 0.25377556681632996\n",
      "335 0.29094165563583374\n",
      "343 0.2429693192243576\n",
      "400 0.2538072168827057\n",
      "417 0.23600360751152039\n",
      "421 0.24806241691112518\n",
      "Component 1:\n",
      "111 0.23795640468597412\n",
      "156 0.29115042090415955\n",
      "Component 2:\n",
      "23 0.32223719358444214\n",
      "156 0.39989417791366577\n",
      "Component 3:\n",
      "23 0.915780246257782\n",
      "66 0.24019086360931396\n",
      "111 0.3680082857608795\n",
      "156 1.3047045469284058\n",
      "162 0.2558390498161316\n",
      "193 0.25004395842552185\n",
      "209 0.22625890374183655\n",
      "271 0.4073232412338257\n",
      "334 0.23126104474067688\n",
      "378 0.2749529480934143\n",
      "394 0.25855162739753723\n",
      "410 0.24639733135700226\n",
      "473 0.2546446621417999\n",
      "503 0.30264389514923096\n",
      "Component 4:\n",
      "Component 5:\n",
      "Component 6:\n",
      "14 0.3215298354625702\n",
      "56 0.3693719506263733\n",
      "89 0.2253105342388153\n",
      "98 0.4586580991744995\n",
      "111 1.0191479921340942\n",
      "271 0.37807178497314453\n",
      "369 0.3085884153842926\n",
      "410 0.38648101687431335\n",
      "503 0.4295293986797333\n",
      "Component 7:\n",
      "Component 8:\n",
      "Component 9:\n",
      "23 0.22006076574325562\n",
      "47 0.4983331859111786\n",
      "89 0.24302326142787933\n",
      "128 0.22135122120380402\n",
      "186 0.3102020025253296\n",
      "263 0.26080626249313354\n",
      "367 0.23302249610424042\n",
      "390 0.26373472809791565\n",
      "416 0.2303304672241211\n",
      "457 0.21987216174602509\n",
      "Component 10:\n",
      "Component 11:\n",
      "Component 12:\n",
      "488 0.2371566742658615\n",
      "Component 13:\n",
      "Component 14:\n",
      "Component 15:\n",
      "total neurons: 52\n"
     ]
    }
   ],
   "source": [
    "neurons_to_ablate = {}\n",
    "total_neurons = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    neurons_to_ablate[submodules[component_idx]] = []\n",
    "    for idx in (effect.act > 0.2135).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        neurons_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_neurons += 1\n",
    "print(f\"total neurons: {total_neurons}\")\n",
    "\n",
    "neurons_to_ablate = {\n",
    "    submodule : n_hot([neuron_idx], dim=512) for submodule, neuron_idx in neurons_to_ablate.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts_abl(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x[...,neurons_to_ablate[submodule]] = x.mean(dim=(0,1))[...,neurons_to_ablate[submodule]] # mean ablation\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = x\n",
    "            else:\n",
    "                submodule.output = x\n",
    "\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "    return act.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0206 15:24:25.609000 5457 torch/fx/experimental/symbolic_shapes.py:6307] failed during evaluate_expr(Eq(4*u0, 0), hint=None, size_oblivious=False, forcing_spec=False\n",
      "E0206 15:24:25.611000 5457 torch/fx/experimental/recording.py:299] failed while running evaluate_expr(*(Eq(4*u0, 0), None), **{'fx_node': False})\n"
     ]
    },
    {
     "ename": "GuardOnDataDependentSymNode",
     "evalue": "Could not guard on data-dependent expression Eq(4*u0, 0) (unhinted: Eq(4*u0, 0)).  (Size-like symbols: u0)\n\nATTENTION: guard_size_oblivious would fix the error, evaluating expression to False.\nMaybe you need to add guard_size_oblivious to framework code, see doc below for more guidance.\n\nCaused by: (utils/_stats.py:21 in wrapper)\nFor more information, run with TORCH_LOGS=\"dynamic\"\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\"\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGuardOnDataDependentSymNode\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmbiguous test accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mtest_probe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_acts_abl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m batches \u001b[38;5;241m=\u001b[39m get_data(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ambiguous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround truth accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, test_probe(probe, get_acts_abl, batches\u001b[38;5;241m=\u001b[39mbatches, label_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[0;32mIn[38], line 42\u001b[0m, in \u001b[0;36mtest_probe\u001b[0;34m(probe, get_acts, label_idx, batches, seed)\u001b[0m\n\u001b[1;32m     40\u001b[0m text \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     41\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[label_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 42\u001b[0m acts \u001b[38;5;241m=\u001b[39m \u001b[43mget_acts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m logits \u001b[38;5;241m=\u001b[39m probe(acts)\n\u001b[1;32m     44\u001b[0m preds \u001b[38;5;241m=\u001b[39m (logits \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m, in \u001b[0;36mget_acts_abl\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_acts_abl\u001b[39m(text):\n\u001b[0;32m----> 2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracer_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubmodules\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msubmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/nnsight/contexts/Runner.py:41\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"On exit, run and generate using the model whether locally or on the server.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremote:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_server()\n",
      "Cell \u001b[0;32mIn[61], line 7\u001b[0m, in \u001b[0;36mget_acts_abl\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tuple[submodule]:\n\u001b[1;32m      6\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mneurons_to_ablate\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,neurons_to_ablate[submodule]] \u001b[38;5;66;03m# mean ablation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tuple[submodule]:\n\u001b[1;32m      9\u001b[0m     submodule\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m][:] \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/nnsight/tracing/Proxy.py:93\u001b[0m, in \u001b[0;36mProxy.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Union[Proxy, Any], value: Union[Self, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/nnsight/tracing/Node.py:148\u001b[0m, in \u001b[0;36mNode.add\u001b[0;34m(self, target, value, args, kwargs, name)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Proxy(\n\u001b[1;32m    138\u001b[0m         Node(\n\u001b[1;32m    139\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m         )\n\u001b[1;32m    146\u001b[0m     )\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/nnsight/tracing/Graph.py:150\u001b[0m, in \u001b[0;36mGraph.add\u001b[0;34m(self, target, value, args, kwargs, name)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m FakeCopyMode(fake_mode):\n\u001b[1;32m    146\u001b[0m             proxy_args, proxy_kwargs \u001b[38;5;241m=\u001b[39m Node\u001b[38;5;241m.\u001b[39mprepare_inputs(\n\u001b[1;32m    147\u001b[0m                 (_args, _kwargs), proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    148\u001b[0m             )\n\u001b[0;32m--> 150\u001b[0m             value \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mproxy_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mproxy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m target_name \u001b[38;5;241m=\u001b[39m target \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m target\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_idx:\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2722\u001b[0m, in \u001b[0;36mFakeCopyMode.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   2720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2721\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 2722\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/utils/_stats.py:21\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1276\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1273\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_dispatch_mode(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TorchDispatchModeKey\u001b[38;5;241m.\u001b[39mFAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m ), func\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake tensor raised TypeError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1816\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_impl(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1386\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     FakeTensorMode\u001b[38;5;241m.\u001b[39mcache_bypasses[e\u001b[38;5;241m.\u001b[39mreason] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m _UNASSIGNED:\n\u001b[0;32m-> 1386\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2067\u001b[0m, in \u001b[0;36mFakeTensorMode._dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   2063\u001b[0m         avoiding_device_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;66;03m# Recompute flat_arg_fake_tensors here again in case some of the inputs\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;66;03m# were real tensors and fakified in validate_and_convert_non_fake_tensors\u001b[39;00m\n\u001b[0;32m-> 2067\u001b[0m (flat_args, flat_arg_fake_tensors) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_and_convert_non_fake_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m args, kwargs  \u001b[38;5;66;03m# Invalidated\u001b[39;00m\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;66;03m# The current constant handling only support tracing systems\u001b[39;00m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;66;03m# (aot autograd, torchdynamo) where each operation is run consecutively.\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;66;03m# Because each operation is run in order, we can trace out and support\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2080\u001b[0m \n\u001b[1;32m   2081\u001b[0m \u001b[38;5;66;03m# We dispatch size/stride/numel on the FakeTensor not its constant, so bail on inplace_view\u001b[39;00m\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2465\u001b[0m, in \u001b[0;36mFakeTensorMode.validate_and_convert_non_fake_tensors\u001b[0;34m(self, func, converter, flat_args, args_spec)\u001b[0m\n\u001b[1;32m   2462\u001b[0m     flat_arg_fake_tensors\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m   2463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m-> 2465\u001b[0m validated_args \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validated_args, flat_arg_fake_tensors\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2465\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2462\u001b[0m     flat_arg_fake_tensors\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m   2463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m-> 2465\u001b[0m validated_args \u001b[38;5;241m=\u001b[39m [\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m flat_args]\n\u001b[1;32m   2466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validated_args, flat_arg_fake_tensors\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2458\u001b[0m, in \u001b[0;36mFakeTensorMode.validate_and_convert_non_fake_tensors.<locals>.validate\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2452\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(flat_args, args_spec)\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m   2454\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease convert all Tensors to FakeTensors first or instantiate FakeTensorMode \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2455\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_non_fake_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_call(func,\u001b[38;5;250m \u001b[39margs,\u001b[38;5;250m \u001b[39mkwargs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2456\u001b[0m         )\n\u001b[0;32m-> 2458\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_real_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2460\u001b[0m     out \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:388\u001b[0m, in \u001b[0;36mFakeTensorConverter.from_real_tensor\u001b[0;34m(self, fake_mode, t, make_constant, shape_env, source, symbolic_context, trace)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m no_dispatch():\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m FakeTensor(\n\u001b[1;32m    380\u001b[0m             fake_mode,\n\u001b[1;32m    381\u001b[0m             make_meta_t(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m             constant\u001b[38;5;241m=\u001b[39mconstant,\n\u001b[1;32m    386\u001b[0m         )\n\u001b[0;32m--> 388\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_converter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmk_fake_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43msymbolic_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbolic_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedFakeTensorException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta converter nyi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/meta_utils.py:1874\u001b[0m, in \u001b[0;36mMetaConverter.__call__\u001b[0;34m(self, t, shape_env, callback, source, symbolic_context, trace)\u001b[0m\n\u001b[1;32m   1869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m st \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1870\u001b[0m         exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m   1871\u001b[0m             torch\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpyfunctorch\u001b[38;5;241m.\u001b[39mtemporarily_clear_interpreter_stack()\n\u001b[1;32m   1872\u001b[0m         )\n\u001b[0;32m-> 1874\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1878\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m        \u001b[49m\u001b[43msymbolic_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter:\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# NB: Cannot directly use Parameter constructor\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;66;03m# because that would force a detach, not desirable\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     r\u001b[38;5;241m.\u001b[39m_is_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/_subclasses/meta_utils.py:1779\u001b[0m, in \u001b[0;36mMetaConverter.meta_tensor\u001b[0;34m(self, t, shape_env, callback, source, symbolic_context)\u001b[0m\n\u001b[1;32m   1775\u001b[0m assert_metadata_eq(assert_eq, t, r, skip_symbolic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, skip_leaf\u001b[38;5;241m=\u001b[39mskip_leaf)\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;66;03m# Thanks to storage resizing, it's possible to end up with a tensor\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;66;03m# that advertises a real size, but has a storage that actually has zero bytes.\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;66;03m# Need to reflect this in the generated FakeTensor.\u001b[39;00m\n\u001b[0;32m-> 1779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mstorage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1780\u001b[0m     r\u001b[38;5;241m.\u001b[39muntyped_storage()\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_parameter:\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/__init__.py:740\u001b[0m, in \u001b[0;36mSymBool.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py:574\u001b[0m, in \u001b[0;36mSymNode.bool_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbool_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguard_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py:512\u001b[0m, in \u001b[0;36mSymNode.guard_bool\u001b[0;34m(self, file, line)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mguard_bool\u001b[39m(\u001b[38;5;28mself\u001b[39m, file, line):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# TODO: use the file/line for some useful diagnostic on why a\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# guard occurred\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/fx/experimental/recording.py:263\u001b[0m, in \u001b[0;36mrecord_shapeenv_event.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m shape_env \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_env\u001b[38;5;241m.\u001b[39mshould_record_events \u001b[38;5;129;01mor\u001b[39;00m shape_env\u001b[38;5;241m.\u001b[39mis_recording:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# If ShapeEnv is already recording an event, call the wrapped\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# function directly.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# NB: here, we skip the check of whether all ShapeEnv instances\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# are equal, in favor of a faster dispatch.\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retlog(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Retrieve an instance of ShapeEnv.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Assumption: the collection of args and kwargs may not reference\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# different ShapeEnv instances.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m _extract_shape_env_and_assert_equal(args, kwargs)\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py:6303\u001b[0m, in \u001b[0;36mShapeEnv.evaluate_expr\u001b[0;34m(self, orig_expr, hint, fx_node, size_oblivious, forcing_spec)\u001b[0m\n\u001b[1;32m   6291\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m   6292\u001b[0m \u001b[38;5;129m@record_shapeenv_event\u001b[39m(save_tracked_fakes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   6293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_expr\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6300\u001b[0m     forcing_spec: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   6301\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m sympy\u001b[38;5;241m.\u001b[39mBasic:\n\u001b[1;32m   6302\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_expr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6304\u001b[0m \u001b[43m            \u001b[49m\u001b[43morig_expr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_oblivious\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforcing_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforcing_spec\u001b[49m\n\u001b[1;32m   6305\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6306\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   6307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   6308\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed during evaluate_expr(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, hint=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, size_oblivious=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, forcing_spec=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   6309\u001b[0m             orig_expr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6312\u001b[0m             forcing_spec,\n\u001b[1;32m   6313\u001b[0m         )\n",
      "File \u001b[0;32m~/feature_circuit_ari/env_fc/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py:6493\u001b[0m, in \u001b[0;36mShapeEnv._evaluate_expr\u001b[0;34m(self, orig_expr, hint, fx_node, size_oblivious, forcing_spec)\u001b[0m\n\u001b[1;32m   6490\u001b[0m         ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   6492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m-> 6493\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_data_dependent_error(\n\u001b[1;32m   6494\u001b[0m             expr\u001b[38;5;241m.\u001b[39mxreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_to_val),\n\u001b[1;32m   6495\u001b[0m             expr,\n\u001b[1;32m   6496\u001b[0m             size_oblivious_result\u001b[38;5;241m=\u001b[39msize_oblivious_result,\n\u001b[1;32m   6497\u001b[0m         )\n\u001b[1;32m   6498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6499\u001b[0m     expr \u001b[38;5;241m=\u001b[39m new_expr\n",
      "\u001b[0;31mGuardOnDataDependentSymNode\u001b[0m: Could not guard on data-dependent expression Eq(4*u0, 0) (unhinted: Eq(4*u0, 0)).  (Size-like symbols: u0)\n\nATTENTION: guard_size_oblivious would fix the error, evaluating expression to False.\nMaybe you need to add guard_size_oblivious to framework code, see doc below for more guidance.\n\nCaused by: (utils/_stats.py:21 in wrapper)\nFor more information, run with TORCH_LOGS=\"dynamic\"\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\"\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9891391396522522\n",
      "Accuracy for (0, 1): 0.3658280074596405\n",
      "Accuracy for (1, 0): 0.5115207433700562\n",
      "Accuracy for (1, 1): 0.9937267303466797\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline feature performance\n",
    "from paper: Instead of relying on human judgement to evaluate whether a\n",
    "feature should be ablated, we zero-ablate the 55 features from our circuit that are\n",
    "most causally implicated in spurious feature accuracy on the balanced set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:04<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# get features which are most useful for predicting gender label\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if running_nodes is None:\n",
    "            running_nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                running_nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in running_nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "946 0.25006115436553955\n",
      "7392 0.6816262602806091\n",
      "17846 0.2601848244667053\n",
      "28533 0.34427016973495483\n",
      "29476 0.14480449259281158\n",
      "31467 0.3418383300304413\n",
      "32081 0.1740640252828598\n",
      "32469 1.2141445875167847\n",
      "Component 1:\n",
      "Component 2:\n",
      "3842 0.1846996247768402\n",
      "10258 0.2082471251487732\n",
      "13387 0.2748056948184967\n",
      "18382 0.11551540344953537\n",
      "19369 0.11571113020181656\n",
      "28127 0.9595451354980469\n",
      "30518 0.20731867849826813\n",
      "31645 0.2460571974515915\n",
      "Component 3:\n",
      "1022 0.4973563551902771\n",
      "3122 0.20655781030654907\n",
      "9651 0.4463863968849182\n",
      "10060 2.1757450103759766\n",
      "18967 0.9766783118247986\n",
      "22084 0.16898095607757568\n",
      "23898 0.27730074524879456\n",
      "26504 0.1362760365009308\n",
      "29626 0.3603300452232361\n",
      "Component 4:\n",
      "Component 5:\n",
      "24159 0.30897560715675354\n",
      "25018 0.3229933977127075\n",
      "Component 6:\n",
      "4592 0.2170836329460144\n",
      "8920 0.5713692307472229\n",
      "9877 0.2788275182247162\n",
      "12128 0.5513262152671814\n",
      "12436 0.20832858979701996\n",
      "15017 3.22841739654541\n",
      "26204 0.12446071207523346\n",
      "30248 0.6778391599655151\n",
      "Component 7:\n",
      "13570 0.11081066727638245\n",
      "27472 1.0406968593597412\n",
      "Component 8:\n",
      "Component 9:\n",
      "1995 0.8690407872200012\n",
      "9128 1.5634136199951172\n",
      "11656 0.1676454246044159\n",
      "12440 0.1573600023984909\n",
      "29206 0.18726511299610138\n",
      "29295 0.3985458016395569\n",
      "30263 0.12636373937129974\n",
      "Component 10:\n",
      "2959 0.8676270842552185\n",
      "19128 0.28398358821868896\n",
      "22029 0.13728690147399902\n",
      "Component 11:\n",
      "Component 12:\n",
      "19558 1.3080941438674927\n",
      "23545 0.3948901295661926\n",
      "27334 0.1667580008506775\n",
      "Component 13:\n",
      "22821 0.11310113966464996\n",
      "31101 0.49193111062049866\n",
      "Component 14:\n",
      "Component 15:\n",
      "9766 0.17570216953754425\n",
      "12420 1.1634024381637573\n",
      "30220 0.2979702651500702\n",
      "total features: 55\n"
     ]
    }
   ],
   "source": [
    "top_feats_to_ablate = {}\n",
    "total_features = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    top_feats_to_ablate[submodules[component_idx]] = []\n",
    "    for idx in (effect > 0.1107).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        top_feats_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_features += 1\n",
    "print(f\"total features: {total_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats_to_ablate = {\n",
    "    submodule : n_hot(feats) for submodule, feats in top_feats_to_ablate.items()\n",
    "}\n",
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, top_feats_to_ablate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9312267303466797\n",
      "Ground truth accuracy: 0.8894008994102478\n",
      "Spurious accuracy: 0.539170503616333\n"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9735883474349976\n",
      "Accuracy for (0, 1): 0.9285767674446106\n",
      "Accuracy for (1, 0): 0.7718893885612488\n",
      "Accuracy for (1, 1): 0.8894051909446716\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining probe on activations after ablating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9572490453720093\n",
      "Ground truth accuracy: 0.9308755993843079\n",
      "Unintended feature accuracy: 0.5195852518081665\n"
     ]
    }
   ],
   "source": [
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, feats_to_ablate)\n",
    "\n",
    "new_probe, _ = train_probe(get_acts_abl, label_idx=0)\n",
    "print('Ambiguous test accuracy:', test_probe(new_probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison  \n",
    "Probe results:  \n",
    "Ambiguous test accuracy: 0.9955855011940002  \n",
    "Ground truth accuracy: 0.6186636090278625  \n",
    "Unintended feature accuracy: 0.8744239807128906  \n",
    "\n",
    "After ablating features:  \n",
    "Ambiguous test accuracy: 0.926347553730011  \n",
    "Ground truth accuracy: 0.8853686451911926  \n",
    "Spurious accuracy: 0.5397465229034424  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9477938413619995\n",
      "Accuracy for (0, 1): 0.8895981907844543\n",
      "Accuracy for (1, 0): 0.9423962831497192\n",
      "Accuracy for (1, 1): 0.9679368138313293\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: 0 hours, 4 minutes, 31.49 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Convert the execution time to hours, minutes, and seconds\n",
    "hours, remainder = divmod(execution_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(f\"Total execution time: {int(hours)} hours, {int(minutes)} minutes, {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_fc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
