{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: \n",
    "\n",
    "- Run gemma on vast.ai\n",
    "- modify the embedding_analysis classes in order to work on gemma structure\n",
    "- test the gemma IT as well \"google/gemma-2-2b-it\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# filepath: vscode-notebook-cell:/home/ailab/Code/MechInt/feature-circuits/experiments/semantic_dispersion.ipynb#Y103sZmlsZQ%3D%3D\n",
    "import sys\n",
    "sys.path.append('/home/ailab/Code/MechInt/feature-circuits/experiments')\n",
    "\n",
    "from embedding_analysis import BlocksEmbeddingAnalysis\n",
    "from embedding_analysis import EmbeddingAnalysis\n",
    "import nnsight\n",
    "from nnsight import NNsight\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "device = 'cuda:0'\n",
    "gemma = LanguageModel(\"google/gemma-2-2b\", device_map=\"cuda\")#, device_map=\"auto\")\n",
    "print(gemma)\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage of the class\n",
    "# text = \"Your input text here\"\n",
    "# analysis = BlocksEmbeddingAnalysis(llm, text)\n",
    "# analysis.analyze_through_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPT tokens in text: 9\n",
      "Number of GPT tokens in high variance text: 35\n",
      "Number of GPT tokens in low variance text: 26\n"
     ]
    }
   ],
   "source": [
    "# Input string\n",
    "text = \"The Eiffel Tower is in the city of\"\n",
    "# High variance text\n",
    "high_variance_text = \"\"\"The quantum mechanics lecture covered topics from wave-particle duality \n",
    "to Schr√∂dinger's cat, while the culinary class next door was busy preparing a variety of exotic dishes.\"\"\"\n",
    "\n",
    "# Low variance text\n",
    "low_variance_text = \"\"\"The cat sat on the mat, purring softly as it basked in the warm sunlight streaming \n",
    "through the window.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Number of tokens in text: {len(gemma.tokenizer.encode(text))}\")\n",
    "\n",
    "print(f\"Number of tokens in high variance text: {len(gemma.tokenizer.encode(high_variance_text))}\")\n",
    "print(f\"Number of tokens in low variance text: {len(gemma.tokenizer.encode(low_variance_text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7c4a8a402f427bab8e5f72fcec7d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NNsightError",
     "evalue": "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 5.92 GiB of which 40.81 MiB is free. Including non-PyTorch memory, this process has 5.17 GiB memory in use. Process 161104 has 724.00 MiB memory in use. Of the allocated memory 5.03 GiB is allocated by PyTorch, and 62.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"/home/ailab/python_venv/feature_circuits/lib/python3.11/site-packages/nnsight/tracing/graph/node.py\", line 289, in execute",
      "    self.target.execute(self)",
      "  File \"/home/ailab/python_venv/feature_circuits/lib/python3.11/site-packages/nnsight/intervention/contexts/interleaving.py\", line 159, in execute",
      "    graph.model.interleave(interleaver, *invoker_args, fn=method,**kwargs, **invoker_kwargs)",
      "  File \"/home/ailab/python_venv/feature_circuits/lib/python3.11/site-packages/nnsight/modeling/mixins/meta.py\", line 49, in interleave",
      "    self.dispatch()",
      "  File \"/home/ailab/python_venv/feature_circuits/lib/python3.11/site-packages/nnsight/modeling/mixins/meta.py\", line 41, in dispatch",
      "    self._model = self._load(*self.args, **self.kwargs)",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/home/ailab/python_venv/feature_circuits/lib/python3.11/site-packages/nnsight/modeling/language.py\", line 175, in _load",
      "    model = self.automodel.from_pretrained(repo_id, config=self.config, **kwargs)",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/home/ailab/python_venv/feature_circuits/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained",
      "    return model_class.from_pretrained(",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/home/ailab/python_venv/feature_circuits/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4224, in from_pretrained",
      "    ) = cls._load_pretrained_model(",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/home/ailab/python_venv/feature_circuits/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4794, in _load_pretrained_model",
      "    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/home/ailab/python_venv/feature_circuits/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 873, in _load_state_dict_into_meta_model",
      "    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)",
      "  File \"/home/ailab/python_venv/feature_circuits/lib/python3.11/site-packages/accelerate/utils/modeling.py\", line 330, in set_module_tensor_to_device",
      "    new_value = value.to(device)",
      "                ^^^^^^^^^^^^^^^^",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 5.92 GiB of which 40.81 MiB is free. Including non-PyTorch memory, this process has 5.17 GiB memory in use. Process 161104 has 724.00 MiB memory in use. Of the allocated memory 5.03 GiB is allocated by PyTorch, and 62.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "",
      "During handling of the above exception, another exception occurred:",
      "",
      "Traceback (most recent call last):",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main",
      "    ",
      "  File \"<frozen runpy>\", line 88, in _run_code",
      "    ",
      "",
      "NNsightError: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 5.92 GiB of which 40.81 MiB is free. Including non-PyTorch memory, this process has 5.17 GiB memory in use. Process 161104 has 724.00 MiB memory in use. Of the allocated memory 5.03 GiB is allocated by PyTorch, and 62.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "with gemma.trace(text):\n",
    "\n",
    "    # Access the last layer using h[-1] as it's a ModuleList\n",
    "    # Access the first index of .output as that's where the hidden states are.\n",
    "\n",
    "    # token_embed = llm.transformer.wte.output.save()    \n",
    "    # pos_embed = llm.transformer.wpe.output.save()\n",
    "    # input_embed = llm.transformer.drop.input.save()\n",
    "    # #llm.transformer.h[-1].mlp.output[0][:] = 0\n",
    "    # ln_0 = llm.transformer.h[0].ln_1.output.save()\n",
    "    # #attn_0 = llm.transformer.h[0].attn.output.save()\n",
    "\n",
    "    # # Access the residual stream after the MLP of the 3rd GPT block\n",
    "    # residual_after_mlp_3 = llm.transformer.h[2].mlp.output.save()\n",
    "\n",
    "    # Logits come out of model.lm_head and we apply argmax to get the predicted token ids.\n",
    "    token_ids = gemma.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "print(\"\\nToken IDs:\", token_ids)\n",
    "#print(\"type:\", type(token_embed))\n",
    "\n",
    "# Apply the tokenizer to decode the ids into words after the tracing context.\n",
    "print(\"Prediction:\", gemma.tokenizer.decode(token_ids[0][-1]))\n",
    "#print(\"Residual after MLP of 3rd GPT block:\", residual_after_mlp_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature_circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
